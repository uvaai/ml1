{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Predicting shipwreck survival\n",
    "\n",
    "In this week's assignment you will implement a classification algorithm named Logistic Regression. This sounds very counter-intuitive, as regression is something entirely different than classification, but the Logistic Regression model is actually estimating (\"regressing\") the probability (which is a continuous value) that data can be assigned to a specific class out of a set of classes. We will focus on so-called binary classification problems, wherein there are two possible classes. Some of the examples of binary classification problems that Logistic Regression is able to solve are: \n",
    "\n",
    "- Email; spam or not spam \n",
    "- Online transactions; fraud or not fraud\n",
    "- Tumor; malignant or benign \n",
    "\n",
    "In this notebook, we will be using the classic *Titanic* dataset. This data consists of demographic and traveling information for 891 of the Titanic's passengers, and the ultimate goal is to predict which of these passengers survived. Here is a summary of the data set's attributes:\n",
    "- PassengerId: passenger ID assigned in this dataset\n",
    "- Survived: A Boolean indicating whether the passenger survived or not (0 = No; 1 = Yes); this is our target\n",
    "- Pclass: Passenger class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "- Name: field containing the name and title of the passenger\n",
    "- Sex: male/female\n",
    "- Age: age of the passenger in years\n",
    "- SibSp: number of siblings/spouses aboard\n",
    "- Parch: number of parents/children aboard\n",
    "- Ticket: ticket number\n",
    "- Fare: passenger fare in British Pounds\n",
    "- Cabin: location of the cabin, consisting of a letter indicating the deck, and a cabin number.\n",
    "- Embarked: port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "As you can probably see, the dataset contains a mix of textual, continuous, categorical, and Boolean variables. Before we can get into implementing and applying Logistic Regression, we will have to clean up the data. We will lead you through this process using the tools a data-scientist might use, providing you with the tools that will help you quickly prepare your own data in the future.\n",
    "\n",
    "In the code below, we have loaded the data and show its first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('titanic.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick view\n",
    "\n",
    "From the first five rows we have displayed in the table above, you might be able to see that there are multiple types of variables: we have integers, strings, floats, and a few \"NaN\"s, which are missing values. Lets take a quick look to see what the types of data that we are dealing with actually are. A tool a data-scientist might use here is Panda's [`df.info()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html), which is a method that can be applied to any `DataFrame` that shows information about a DataFrame including the index, datatype for each column, non-null values, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have 891 entries, numbered 0 to 890, with 12 different variables each. Of these 12 variables \"Survived\" will be our target variable. Out of 12 columns, 2 are floats, 5 are integers, and 5 are \"objects\" which in this case means that they are strings. Out of all of the columns, there are 3 that have some missing values. We will have to fix these missing values later.\n",
    "\n",
    "Let's take a better look at what the columns that have integers and floats in them look like. For this, we will use Panda's [`df.describe`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html?highlight=describe#pandas.DataFrame.describe) which is a method that can be applied to any `DataFrame` that shows descriptive statistics that summarize the central tendency, dispersion and shape of a datasetâ€™s distribution, excluding NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a table showing a bunch of statistics for each of the columns. The `count` is the number of values that is not NaN in that specific column. We can also see mean, standard deviation, minimum, maximum, and percentiles for each of the columns. These [percentiles](https://en.wikipedia.org/wiki/Percentile) indicate the value below which a given percentage of the data in the column falls. Let's see what we can conclude from this table and the previous one:\n",
    "\n",
    "- PassengerId is a column that runs from 1 to 891, indicating the index of a passenger in this dataset. This variable has no intrinsic value and might even make our machine learning model find correlations that are no indication of the truth, so we can delete the whole column later.\n",
    "- The mean of Survived indicates that out of the 891 people in our dataset, only ~38% of people survived.\n",
    "- There are more people in Passenger Class 3 than in class 1 and 2 combined.\n",
    "- There is at least one baby in our dataset, with an Age of 0.42, or 5 months. The oldest person is 80 years old.\n",
    "- There are 277 people of which we do not know the Age.\n",
    "- The majority of people has no siblings/spouses aboard.\n",
    "- The majority of people has no children/parents aboard. \n",
    "- There is a big difference in scale in data. Fare can be up to 512 pounds while the number of children/parents on board is only up to 6.  We can also see this in the standard deviations, where the difference is also very large. In his videos, Andrew Ng talks about feature scaling, which is something we must apply here. We will discuss this later.\n",
    "\n",
    "Often, these tables can tell you a lot about the data you are working with, and might provide you with intuitions on what would work best for this dataset. We will spend some more time on analysing the data at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "Let's create a copy of the dataset that does not contain data that we do not need for Logistic Regression. In this case, we will delete the rows containing the Passenger ID, Name, Cabin Number, or Ticket number of each passenger. In a more advanced data processing pipeline we might opt to include this data, but that would include data mining techniques that are beyond the scope of this notebook. Keep in mind that the cabin number, for example, may include information on where the passenger was on the ship, which in turn might give information on if a passenger would have been able to reach a lifeboat timely.\n",
    "\n",
    "Create a new DataFrame called `clean_data` that is a direct copy of our original `data`, but does not include the colums `['PassengerId', 'Cabin','Name','Ticket']`. Take a look at Panda's [`df.drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) which can be used to delete columns from a given `DataFrame`, especially the parameter `axis` is important here. The method returns a `Dataframe` without the columns that were deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "clean_data = data.drop(['PassengerId', 'Cabin','Name','Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis of the data, we concluded that there were 277 people of which we do not know the age. One could say that the absence of information is also information, but in our case there is no way that our model can calculate a prediction when one of the input variables is absent. When you encounter missing values, you have multiple options for dealing with these missing values: you can delete each row that contains a missing values, you can delete the whole column from the DataFrame, or you can replace the missing values. In our case, deleting the rows would not be an option, as we already do not have a lot of data, and we need as much as possible to create an accurate model. Deleting the column would also not be an option, as the age of a person will probably be a good indicator for whether a person survived or not. The third option, replacing the missing values is good, but we have to find a method that will not skew our results. \n",
    "\n",
    "There are multiple ways of replacing missing values, but it is important to find a method that does not affect the outcome of our model too much. As replacing the values randomly will probably create a lot of outliers and will probably result in our model not being able to learn what correlations there are within the data, we will use a method that replaces missing values with values that are as generic as possible. An easy way to do this is to replace all missing values with a value that is inferred from the data that is available.\n",
    "\n",
    "In the cell below, we use Panda's [`df.fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html?highlight=fillna#pandas.DataFrame.fillna) to replace every missing value in the age column of `clean_data` with the average age of all non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>13.002015</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived      Pclass         Age       SibSp       Parch        Fare\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000\n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
       "std      0.486592    0.836071   13.002015    1.102743    0.806057   49.693429\n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    2.000000   22.000000    0.000000    0.000000    7.910400\n",
       "50%      0.000000    3.000000   29.699118    0.000000    0.000000   14.454200\n",
       "75%      1.000000    3.000000   35.000000    1.000000    0.000000   31.000000\n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_data['Age'] = clean_data['Age'].fillna(clean_data['Age'].mean())\n",
    "\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all missing alues in the column \"Age\" have now been replaced with the average. This means that the average of the column has not changed. However, this solution is still not ideal. We have changed the standard deviation and the percentiles, which indicates that we have changed the entire distribution of the data. Sadly there is no way to recover the real data, and for now, we will have to settle with this solution.\n",
    "\n",
    "Finally, we have to transform the columns \"Sex\" and \"Embarked\" from categorical data into a type of data that the model can handle. The column Sex has two categories, male or female, while the port where passengers Embarked has three categories: C = Cherbourg, Q = Queenstown, or S = Southampton. In the previous notebook, we have introduced you to one-hot encoding, which is exactly what we will use here. Panda's has a method (as is to be expected) named [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) that transforms data from categorical to a one-hot encoded set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  891 non-null    int64  \n",
      " 1   Pclass    891 non-null    int64  \n",
      " 2   Age       891 non-null    float64\n",
      " 3   SibSp     891 non-null    int64  \n",
      " 4   Parch     891 non-null    int64  \n",
      " 5   Fare      891 non-null    float64\n",
      " 6   male      891 non-null    uint8  \n",
      " 7   C         891 non-null    uint8  \n",
      " 8   Q         891 non-null    uint8  \n",
      " 9   S         891 non-null    uint8  \n",
      "dtypes: float64(2), int64(4), uint8(4)\n",
      "memory usage: 45.4 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare  male  C  Q  S\n",
       "0         0       3  22.0      1      0   7.2500     1  0  0  1\n",
       "1         1       1  38.0      1      0  71.2833     0  1  0  0\n",
       "2         1       3  26.0      0      0   7.9250     0  0  0  1\n",
       "3         1       1  35.0      1      0  53.1000     0  0  0  1\n",
       "4         0       3  35.0      0      0   8.0500     1  0  0  1"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can drop one of the sexes, as being one of the sexes infers that the passenger can no longer be the other\n",
    "sex = pd.get_dummies(clean_data['Sex'], drop_first=True)\n",
    "\n",
    "# This is not the case for port of embarkment, as there are three possibilities here\n",
    "embark = pd.get_dummies(clean_data['Embarked'])\n",
    "\n",
    "# We no longer need these columns, as we will replace them soon\n",
    "clean_data = clean_data.drop(['Sex','Embarked'],axis=1)\n",
    "\n",
    "# Create the new dataframe with the columns we inferred from the categories in Sex and Embarked\n",
    "clean_data = pd.concat([clean_data, sex, embark],axis=1)\n",
    "\n",
    "clean_data.info()\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final preparations\n",
    "\n",
    "Now that all data is numerical, we can continue to the next step. We have already observed that there is a big difference in scale in data. In his videos, Andrew Ng talks about feature scaling. In feature scaling you make sure that the different features in your dataset take on similar ranges of values. The idea behind this is that after you apply feature scaling, gradient descent will converge more quickly. \n",
    "\n",
    "Scipy's `stat` module has some interesting methods to do this, including the `zscore` method which you might recognise from Andrew's video:\n",
    "\n",
    "$$ Z = \\frac{X - \\mu}{\\sigma} $$\n",
    "\n",
    "The basic principle here is that it transforms every value in $X$ to a value that indicates how many standard deviations the value originally was away from the mean of the data. For example: a $Z$-score of $-2$ would indicate that the value was 2 standard deviations below the average, while a $Z$-score of $0$ would indicate that the value was\n",
    "\n",
    "In the code below, we transform each of the non-categorical datacolumns using the `zscore` method, by applying it to each of the elements within. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>4.386066e-17</td>\n",
       "      <td>-3.987333e-17</td>\n",
       "      <td>1.594933e-17</td>\n",
       "      <td>5.283216e-17</td>\n",
       "      <td>3.987333e-18</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.566107e+00</td>\n",
       "      <td>-2.253155e+00</td>\n",
       "      <td>-4.745452e-01</td>\n",
       "      <td>-4.736736e-01</td>\n",
       "      <td>-6.484217e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.693648e-01</td>\n",
       "      <td>-5.924806e-01</td>\n",
       "      <td>-4.745452e-01</td>\n",
       "      <td>-4.736736e-01</td>\n",
       "      <td>-4.891482e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.273772e-01</td>\n",
       "      <td>-2.232906e-16</td>\n",
       "      <td>-4.745452e-01</td>\n",
       "      <td>-4.736736e-01</td>\n",
       "      <td>-3.573909e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.273772e-01</td>\n",
       "      <td>4.079260e-01</td>\n",
       "      <td>4.327934e-01</td>\n",
       "      <td>-4.736736e-01</td>\n",
       "      <td>-2.424635e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.273772e-01</td>\n",
       "      <td>3.870872e+00</td>\n",
       "      <td>6.784163e+00</td>\n",
       "      <td>6.974147e+00</td>\n",
       "      <td>9.667167e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived        Pclass           Age         SibSp         Parch  \\\n",
       "count  891.000000  8.910000e+02  8.910000e+02  8.910000e+02  8.910000e+02   \n",
       "mean     0.383838  4.386066e-17 -3.987333e-17  1.594933e-17  5.283216e-17   \n",
       "std      0.486592  1.000562e+00  1.000562e+00  1.000562e+00  1.000562e+00   \n",
       "min      0.000000 -1.566107e+00 -2.253155e+00 -4.745452e-01 -4.736736e-01   \n",
       "25%      0.000000 -3.693648e-01 -5.924806e-01 -4.745452e-01 -4.736736e-01   \n",
       "50%      0.000000  8.273772e-01 -2.232906e-16 -4.745452e-01 -4.736736e-01   \n",
       "75%      1.000000  8.273772e-01  4.079260e-01  4.327934e-01 -4.736736e-01   \n",
       "max      1.000000  8.273772e-01  3.870872e+00  6.784163e+00  6.974147e+00   \n",
       "\n",
       "               Fare        male           C           Q           S  \n",
       "count  8.910000e+02  891.000000  891.000000  891.000000  891.000000  \n",
       "mean   3.987333e-18    0.647587    0.188552    0.086420    0.722783  \n",
       "std    1.000562e+00    0.477990    0.391372    0.281141    0.447876  \n",
       "min   -6.484217e-01    0.000000    0.000000    0.000000    0.000000  \n",
       "25%   -4.891482e-01    0.000000    0.000000    0.000000    0.000000  \n",
       "50%   -3.573909e-01    1.000000    0.000000    0.000000    1.000000  \n",
       "75%   -2.424635e-02    1.000000    0.000000    0.000000    1.000000  \n",
       "max    9.667167e+00    1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "non_categorical = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "clean_data[non_categorical] = clean_data[non_categorical].apply(zscore)\n",
    "\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that we have adjusted with our `zscore` function now all have a mean that is very close to zero, and a standard deviation that is very close to one. This means that our data is now sufficiently of equal scale and mean-centered.\n",
    "\n",
    "The final thing that we have to do to prepare the data, is separate the input variables from the target variable, and split the dataset into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into target \"y\" and input \"X\"\n",
    "y = clean_data['Survived']\n",
    "X = clean_data.drop('Survived', axis=1)\n",
    "\n",
    "#Split the data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic function\n",
    "\n",
    "For logistic regression, we require a function that generates probabilities; a function that gives outputs between 0 and 1 for **all** inputs. There are many functions that meet this description, but the one that is used in logistic regression is the logistic function:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "Implement the function `logistic_func` that can either take a single value `z`, or an array of values `z`, and compute the *Logistic* function for each.\n",
    "\n",
    "Then, write some code that can plot the results for the logistic function between $-5$ and $5$. Use Numpy's [`np.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html?highlight=linspace#numpy.linspace) to generate a range of $z$ values and use your `logistic_func` to calculate the corresponding $g$ values. Can you see why this function is also sometimes called the *Sigmoid* function? (The term \"sigmoid\" means S-shaped.)\n",
    "\n",
    "*Hint:* Numpy built-in functions and basic arithmetic operations work on both Numpy arrays and single values. When the function detects that its input is a Numpy array, the operation it applies will be applied to each element in the array seperately, the result being a Numpy array of the same shape as the input. This is called *element-wise application*. Write your `logistic_func` in such a way that it can compute $g$ values for scalars or for entire vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logistic_func(z):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "assert logistic_func(0) == 0.5\n",
    "assert logistic_func(1) == np.e / (1 + np.e)\n",
    "assert np.allclose(logistic_func(np.array([0,1])), np.array([0.5, np.e/ (1 + np.e)]))\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "z = np.linspace(-5, 5)\n",
    "g = logistic_func(z)\n",
    "\n",
    "plt.plot(z, g)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis function\n",
    "\n",
    "We now have a method of transforming our inputs into probabilities, and we can start working on the hypothesis function which will generate an output given a set of inputs $X$ -- in our case the features we created from the Titanic data set -- and a given set of weights $\\theta$ -- which we will try to learn from our dataset.\n",
    "\n",
    "In the case of logistic regression, we can use the logistic function $g$ to transform the hypothesis we used in multivariate linear regression to a hypothesis function that only generates probabilities (results from 0 to 1):\n",
    "\n",
    "$$ h_\\theta(x^1) = g(\\theta^Tx^1) = g(z^1) = \\frac{1}{1+e^{-z^1}} = \\frac{1}{1+e^{-\\theta^Tx^1}}$$\n",
    "$$ h_\\theta(x^2) = g(\\theta^Tx^2) = g(z^2) = \\frac{1}{1+e^{-z^2}} = \\frac{1}{1+e^{-\\theta^Tx^2}}$$\n",
    "$$ h_\\theta(x^3) = g(\\theta^Tx^3) = g(z^3) = \\frac{1}{1+e^{-z^3}} = \\frac{1}{1+e^{-\\theta^Tx^3}}$$\n",
    "$$\\dots$$\n",
    "$$ h_\\theta(x^n) = g(\\theta^Tx^n) = g(z^n) = \\frac{1}{1+e^{-z^n}} = \\frac{1}{1+e^{-\\theta^Tx^n}}$$\n",
    "\n",
    "\n",
    "Herein, $z^i$ is the linear combination of the i-th input vector $x^i$ and the parameter vector $\\theta$.\n",
    "\n",
    "Now, in our text on [the logistic function](#The-logistic-function) just above this text, we discussed how Numpy is able to apply functions element-wise. Using the matrix multiplication we have also used in multivariate linear regression, we can generate a column vector of predictions that have not yet been transformed into probabilities: \n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "x_0^1 & x_1^1 & x_2^1 & \\cdots & x_n^1 \\\\ \n",
    "x_0^2 & x_1^2 & x_2^2 & \\cdots & x_n^2 \\\\ \n",
    "x_0^3 & x_1^3 & x_2^3 & \\cdots & x_n^3 \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_0^m & x_1^m & x_2^m & \\cdots & x_n^m \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\\n",
    "\\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{array} \\right]\n",
    "= \\left[\\begin{array}{cccc}\n",
    "z^1\\\\\n",
    "z^2\\\\\n",
    "z^3\\\\\n",
    "\\dots \\\\\n",
    "z^m \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "We can then use our logistic function to apply this function to each of the elements in this column vector of predictions:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cccc}\n",
    "g(z^1)\\\\\n",
    "g(z^2)\\\\\n",
    "g(z^3)\\\\\n",
    "\\dots \\\\\n",
    "g(z^m) \\\\\n",
    "\\end{array} \\right]\n",
    "=\n",
    "\\left[\\begin{array}{c} h_{\\theta}(x^1) \\\\ h_{\\theta}(x^2) \\\\\n",
    "h_{\\theta}(x^3) \\\\ \\vdots \\\\ h_{\\theta}(x^m) \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "Due to Numpy applying the function to each element, instead of the whole vector at once, the only thing that really changes in our programming between the multivariate linear regression model and our new logistic regression model is that we use our logistic function to transform the whole vector of values to be between 0 and 1. To get these prediction values, we can thus just use our old function `linear_model`.\n",
    "\n",
    "First, copy your implementation of `linear_model`, which you have already implemented in the polynomial regression notebook.\n",
    "\n",
    "Then, write the function `add_x0`, which takes a matrix of `X` values, and returns that same matrix with a column of ones added to the front. For this function you should use [np.ones](https://numpy.org/doc/stable/reference/generated/numpy.ones.html) to create the column of ones. Then, take a look at [`np.concatenate`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html?highlight=concat#numpy.concatenate), which enables you to combine the column vector of ones to your matrix `X`.\n",
    "\n",
    "Then write the function `logistic_model`, which takes a matrix of `X` values, a weight vector `theta`, and calculates $h(X)$ using the functions `add_x0`, `logistic_func`, and `linear_model`. Make sure to first add the bias, or the shapes of the matrix `X` and vector `theta` won't match up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X, theta):\n",
    "    ### YOUR (COPIED) SOLUTION HERE\n",
    "    return X.dot(theta)\n",
    "\n",
    "def add_x0(X):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return np.concatenate((np.ones((X.shape[0],1)), X), axis=1)\n",
    "\n",
    "def logistic_model(X, theta):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    X = add_x0(X)\n",
    "    \n",
    "    return logistic_func(linear_model(X, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Now that we can make predictions using an input matrix $X$, and our parameter weights $\\theta$, we can evaluate the cost of our model. To be able to apply gradient descent, we will need a cost function that is convex when we use our logistic model to make predictions. This cost function is explained in great detail by Andrew Ng, so we will not explain it again here. The logistic regression cost function is given as:\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^M y^i log(h_\\theta(X^i)) + (1 - y^i) log(1 - h_\\theta(X^i))$$\n",
    "\n",
    "Implement the function `logistic_cost`. Use your `logistic_model` function to calculate the hypothesis vector $h_\\theta(X)$. Just as with the cost function for Polynomial Regression, it is actually possible to write this whole function using linear algebra. \n",
    "\n",
    "*Hint:* By using `np.log` to apply the logarithm function to every element of a vector, you can calculate the logarithm of every element in the vector $h_\\theta(X)$ at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost(theta, X, y):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    h_X = logistic_model(X, theta)\n",
    "\n",
    "    return -(y.dot(np.log(h_X)) + (1 - y).dot(np.log(1 - h_X))) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Vector and Gradient Descent\n",
    "\n",
    "The way we calculate the partial derivatives for each of the $\\theta$ parameters is the exact same as for Polynomial Regression. Again, this is explained in great detail by Andrew Ng, but the essence is that while the hypothesis function $h$ changes, the derivative does not. As a result, the equations for the $n+1$ partial derivatives for each of the $n+1$ parameters *still* looks like:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_0$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_1$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_2} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_2$$\n",
    "$$\\dots$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_n} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_n$$\n",
    "\n",
    "Copy your implementation of `gradient_vector` and `gradient_descent`, which you have already implemented in the polynomial regression notebook. *Change `gradient_vector` to make use of the new hypothesis function `logistic_model`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6781436388945372\n",
      "[-0.01345543 -0.02384207 -0.00585881 -0.00244681  0.00458074  0.01809032\n",
      " -0.02746011  0.002498   -0.00127184 -0.01479422]\n"
     ]
    }
   ],
   "source": [
    "def gradient_vector(theta, X, y):\n",
    "    ### YOUR (COPIED) SOLUTION HERE\n",
    "    h_X = logistic_model(X, theta)\n",
    "    \n",
    "    X = add_x0(X)\n",
    "    \n",
    "    return (h_X - y).dot(X) / X.shape[0]\n",
    "    \n",
    "def gradient_descent(X, y, theta, alpha, thres=10**-6):\n",
    "    ### YOUR (COPIED) SOLUTION HERE\n",
    "    cost = logistic_cost(theta, X, y)\n",
    "    cost_hat = cost + 1\n",
    "    \n",
    "    while (abs(cost - cost_hat) > thres):\n",
    "        cost = cost_hat\n",
    "        theta = theta - alpha * gradient_vector(theta, X, y)\n",
    "\n",
    "        cost_hat = logistic_cost(theta, X, y)\n",
    "\n",
    "    print(cost)\n",
    "    return theta\n",
    "\n",
    "# Find the theta vector that minimizes the cost function\n",
    "theta = gradient_descent(X_train, y_train, np.zeros(X_train.shape[1] + 1), 10**-5)\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We now have a learned vector of weights $\\theta$ that we can use to make predictions for our test samples `X_test`. We can re-use our hyptohesis function to calculate the probability that each of the samples in `X_test` is a survivor or not: $p(y=1|X_{test},\\theta)$. These estimates of the probability that $y=1$ still need to be transformed into boolean predictions; we predict that either the person has survived $y=1$, or that the person has not survived $y=0$.\n",
    "\n",
    "Write the function `predict`, that uses a matrix of input values `X` and a vector of weights `theta`, and transforms them into a vector of boolean predictions. Use `logistic_model` to generate predictions, and then use a decision boundary of $h_\\theta(x) \\leq 0.5$ to determine when $y=1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return logistic_model(X, theta) >= 0.5\n",
    "\n",
    "predictions = predict(X_test, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining accuracy\n",
    "\n",
    "Of course we would like to see how *accurate* our model is now that we have trained it and generated predictions! Implement the function `calc_accuracy` that accepts a vector of `predictions` and a vector of truth values `y`. The function should return the factor of correct predictions (predictions where the value in `predictions` is equal to the value in `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7126865671641791\n"
     ]
    }
   ],
   "source": [
    "def calc_accuracy(predictions, y):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return sum(predictions == y) / len(y)\n",
    "\n",
    "print(calc_accuracy(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The accuracy we have just determined is of course one of the primary goals of building a model; we generally want a Machine Learning model to be as accurate as possible. However, it does not give much insight into in what way our model was accurate. As an example, let's say that we want to diagnose whether a patient has a virus. Of course we want our model to diagnose as accurately as possible, but we would also prefer that a patient that has the virus was not diagnosed as negative (not having the virus), while the opposite is not as important. Falsely diagnosing a patient as negative could become very harmful for the patient and their environment. \n",
    "\n",
    "To give better insight into the performance of a classification algorithm, typically, a confusion matrix is made. The confusion matrix is a table layout that allows quick identification of the performance of a classification algorithm. Each *row* in the matrix represents the data that got predicted as a specific class, while each *column* represents the data that is actually in a class:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> TP </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> FP </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\">FN</td>\n",
    "            <td style=\"border: 1px solid black;\"><b> TN </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Where:\n",
    "- P = Positive\n",
    "- N = Negative\n",
    "- TP = True Positive, denoting every value that was correctly predicted as positive\n",
    "- FP = False Positive, denoting every value that was predicted as positive but was actually negative\n",
    "- TN = True Negative, denoting every value that was correctly predicted as negative\n",
    "- FN = False Negative, denoting every value that was predicted as negative but was actually positive\n",
    "\n",
    "For example, using the example of diagnosis of patients, let's say that 100 people take a test, and of these people, 80 actually have the virus. We have two different methods of diagnosing the patients, and these are the resulting confusion matrices:\n",
    "\n",
    "\n",
    "##### Method 1\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 56 </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 1 </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 24 </td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 19 </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### Method 2\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 62 </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 10 </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 18 </td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 10 </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Now, we can learn a lot out of this confusion matrix. First, the calculated accuracy of our first virus diagnosis method is 77%, as 56 people are correctly diagnosed as positive, and 19 people are correctly diagnosed as negative.  The calculated accuracy for the second method is 72%. However, using the first method 24 people have been incorrectly diagnosed as negative, while for the second this was 18. Now, arguably, we would want to use method 2, as even though the total accuracy of this method is lower, this method produces fewer (harmfull) false negatives. \n",
    "\n",
    "Implement the function `confusion_matrix` that, given `predictions` and truth values `y`, creates a 2-dimensional Numpy array that contains the number of TP, FP, FN, and TN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[233  69]\n",
      " [166 132]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(predictions, y):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    TP = sum(predictions == y & predictions)\n",
    "    TN = sum(predictions == y & ~predictions)\n",
    "    FP = sum(predictions == 1 & ~y)\n",
    "    FN = sum(predictions == 0 & y)\n",
    "    \n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "print(confusion_matrix(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations and understanding weights\n",
    "\n",
    "To better understand what relations our model has found in the data we will have to take a look at the weights that our model has learned. The relative size of each weight is a direct indication of how important the corresponding feature is in defining the relationship between the input features and the target output. We can even argue why this is the case, as the hypothesis of our model is entirely dependant on the linear combination of the input values and our weights:\n",
    "\n",
    "$$ h_{\\theta}(x^i) = g(x^i_0 \\theta_0 + x^i_1 \\theta_1 + \\dots + x^i_n \\theta_n)$$\n",
    "\n",
    "The logistic function simply \"squashes\" the combined positive values to a maximum of 1, and the combined negative values to a minimum of 0. I.e. a negative weight means that a large value for this feature will actually decrease the chances of survival for a passenger. \n",
    "\n",
    "The input for our function $g$ is composed out of the sums of each input multiplied with its corresponding weight. Since we have made sure that our data all approximately has equal scale by using the `zscore` method, bigger values for a specific weight result in the input corresponding to that weight having more influence on the hypothesis. Similarly, negative values indicate that there is a negative correlation between the input variable and the hypothesis, while positive values indicate a positive correlation.\n",
    "\n",
    "Below, we have used Seaborn to create a barplot that shows each of the weights and their corresponding data column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(X_train.columns, theta[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that the four most important variables are the Passenger Class, the Fare the passenger paid in pounds, whether the person is a male, and whether the person embarked in Southampton. Passenger class is negatively correlated, which means that a being in a higher passenger class results in a lower chance of survival. Paying more fare results in a higher probability of survival. While it is dangerous to draw conclusions from this plot, we might form some theories on why this is the case. One such theory would be that due to the \"Children and females first\" rule the chance of surviving while being a male is lower. Another unrelated theory could be that people who are in first class (and thus paid a higher fare) were on decks that were closer to the top of the boat, where the lifeboats were positioned, and thus had a higher chance of getting to a lifeboat in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Analysis\n",
    "\n",
    "We will take a further look in some of these theories by making some plots using Seaborn. First, let's take a look at how many people have survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Survived',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that approximately ~350 people have survived, while approximately ~550 people have died. This can actually also be seen in the weights our model, as the total absolute value of the negative weights is higher than the total absolute value of the positive weights! Remember that this is only possible since each of our inputs is similarly scaled.\n",
    "\n",
    "Our model found that being a male was negatively correlated to surviving. Let's see if this is also apperant from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Survived',hue='Sex',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that there are more males than females in our dataset. We can also see that approximately ~70% of females survived, while only ~20% of males survived. As our model concluded, there was indeed a larger chance of survival for females when compared to males.\n",
    "\n",
    "Let's take a look at another one of the variables that was negatively correlated; Passenger Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAW90lEQVR4nO3de5CddZ3n8feHkCHMgCIkaEzAoMuOXNNAQ3ACyMKwAuUIw21A5KIp41YhYjnDjo6WAi4M1ojXHWFBFIIsGHVUFhRFkHXMjmCQiImMS8RoWqKEIBqWi0n87h/95KElDekkffp00+9X1anznN/ze37nezTVH37PNVWFJEkAW3W7AEnS6GEoSJJahoIkqWUoSJJahoIkqbV1twvYEpMnT64ZM2Z0uwxJGlPuueeeR6pqymDrxnQozJgxg4ULF3a7DEkaU5L8/LnWuftIktQyFCRJLUNBktQa08cUJKlb1qxZQ19fH0899VS3S3lOkyZNYvr06UycOHHI2xgKkrQZ+vr62H777ZkxYwZJul3OBqqKVatW0dfXx2677Tbk7dx9JEmb4amnnmKnnXYalYEAkISddtppk2cyhoIkbabRGgjrbU59hoIkqWUoSNIwmjBhAj09Pey9996cfPLJPPHEE8/Z94ILLuDDH/7wCFa3cR5oVsfN/uTsbpewyRacu6DbJWiM2nbbbVm0aBEAp59+OldccQXvete7ulzV0DlTkKQOOfTQQ1m6dCkA8+bNY99992XmzJmcccYZG/S96qqrOPDAA5k5cyYnnnhiO8P4whe+wN57783MmTM57LDDAFiyZAkHHXQQPT097LvvvjzwwAPDVrMzBUnqgLVr1/L1r3+do48+miVLlnDxxRezYMECJk+ezKOPPrpB/xNOOIG3vvWtALzvfe/j6quv5txzz+Wiiy7iG9/4BtOmTeOxxx4D4IorruC8887j9NNP5/e//z3r1q0btrqdKUjSMHryySfp6emht7eXXXfdlTlz5nDHHXdw0kknMXnyZAB23HHHDbZbvHgxhx56KPvssw/XX389S5YsAWD27NmcffbZXHXVVe0f/9e85jVccsklfOhDH+LnP/8522677bDV70xBkobRwGMK61XVRk8PPfvss/nKV77CzJkzueaaa7jzzjuB/lnBXXfdxS233EJPTw+LFi3ijW98I7NmzeKWW27hda97HZ/+9Kc54ogjhqV+ZwqS1GFHHnkk8+fPZ9WqVQCD7j5avXo1U6dOZc2aNVx//fVt+09/+lNmzZrFRRddxOTJk1m+fDkPPvggr3zlK3nHO97BG97wBu67775hq9WZgiR12F577cV73/teXvva1zJhwgT2228/rrnmmj/q88EPfpBZs2bxile8gn322YfVq1cDcP755/PAAw9QVRx55JHMnDmTSy+9lM997nNMnDiRl73sZbz//e8ftlpTVcM22Ejr7e0tH7Iz+nlKql6I7r//fvbYY49ul7FRg9WZ5J6q6h2sv7uPJEmtjoVCkklJ7k7ywyRLklzYtF+T5GdJFjWvnqY9ST6RZGmS+5Ls36naJEmD6+QxhaeBI6rq8SQTge8m+Xqz7vyq+uKz+h8D7N68ZgGXN++SpBHSsZlC9Xu8+TixeT3fAYzjgHnNdt8DdkgytVP1SZI21NFjCkkmJFkEPAzcVlV3NasubnYRfTTJNk3bNGD5gM37mrZnjzk3ycIkC1euXNnJ8iVp3OloKFTVuqrqAaYDByXZG3gP8GrgQGBH4O+b7oNd2bHBzKKqrqyq3qrqnTJlSocql6TxaUSuU6iqx5LcCRxdVevvE/t0ks8Cf9d87gN2GbDZdOChkahPkrbUAefPG9bx7vmnMzfa5y1veQs333wzO++8M4sXLx6W7+3k2UdTkuzQLG8L/CXw7+uPE6T/mu/jgfW/5CbgzOYspIOB31bVik7VJ0lj3dlnn82tt946rGN2cqYwFbg2yQT6w2d+Vd2c5I4kU+jfXbQI+C9N/68BxwJLgSeAN3ewNkka8w477DCWLVs2rGN2LBSq6j5gv0HaB71rU/VfWn1Op+qRJG2cVzRLklqGgiSpZShIklreOluShsFQTiEdbqeddhp33nknjzzyCNOnT+fCCy9kzpw5WzSmoSBJY9QNN9ww7GO6+0iS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktT0mVpGHwi4v2Gdbxdn3/j553/fLlyznzzDP51a9+xVZbbcXcuXM577zztvh7DQVJGoO23nprLrvsMvbff39Wr17NAQccwFFHHcWee+65ReO6+0iSxqCpU6ey//77A7D99tuzxx578Mtf/nKLxzUUJGmMW7ZsGffeey+zZs3a4rEMBUkawx5//HFOPPFEPvaxj/GiF71oi8czFCRpjFqzZg0nnngip59+OieccMKwjGkoSNIYVFXMmTOHPfbYg3e9613DNq5nH0nSMNjYKaTDbcGCBVx33XXss88+9PT0AHDJJZdw7LHHbtG4HQuFJJOA7wDbNN/zxar6QJLdgBuBHYEfAGdU1e+TbAPMAw4AVgF/U1XLOlWfJI1lhxxyCP2Pth9endx99DRwRFXNBHqAo5McDHwI+GhV7Q78Blj/RIg5wG+q6j8AH236SZJGUMdCofo93nyc2LwKOAL4YtN+LXB8s3xc85lm/ZFJ0qn6JEkb6uiB5iQTkiwCHgZuA34KPFZVa5sufcC0ZnkasBygWf9bYKdBxpybZGGShStXruxk+ZI07nQ0FKpqXVX1ANOBg4A9BuvWvA82K9hgh1lVXVlVvVXVO2XKlOErVpI0MqekVtVjwJ3AwcAOSdYf4J4OPNQs9wG7ADTrXww8OhL1SZL6dSwUkkxJskOzvC3wl8D9wLeBk5puZwFfbZZvaj7TrL+jOnFoXZL0nDp5ncJU4NokE+gPn/lVdXOSHwM3JvlvwL3A1U3/q4Hrkiylf4Zwagdrk6RhNfuTs4d1vAXnLnje9U899RSHHXYYTz/9NGvXruWkk07iwgsv3OLv7VgoVNV9wH6DtD9I//GFZ7c/BZzcqXok6YVkm2224Y477mC77bZjzZo1HHLIIRxzzDEcfPDBWzSut7mQpDEoCdtttx3Qfw+kNWvWMBxn8RsKkjRGrVu3jp6eHnbeeWeOOuoob50tSePZhAkTWLRoEX19fdx9990sXrx4i8c0FCRpjNthhx04/PDDufXWW7d4LENBksaglStX8thjjwHw5JNP8q1vfYtXv/rVWzyut86WpGGwsVNIh9uKFSs466yzWLduHX/4wx845ZRTeP3rX7/F4xoKkjQG7bvvvtx7773DPq67jyRJLUNBktQyFCRpM43227NtTn2GgiRthkmTJrFq1apRGwxVxapVq5g0adImbeeBZknaDNOnT6evr4/R/LCvSZMmMX369E3axlCQpM0wceJEdtttt26XMezcfSRJahkKkqSWoSBJahkKkqSWoSBJanUsFJLskuTbSe5PsiTJeU37BUl+mWRR8zp2wDbvSbI0yU+SvK5TtUmSBtfJU1LXAn9bVT9Isj1wT5LbmnUfraoPD+ycZE/gVGAv4OXAt5L8x6pa18EaJUkDdGymUFUrquoHzfJq4H5g2vNschxwY1U9XVU/A5YCB3WqPknShkbkmEKSGcB+wF1N09uT3JfkM0le0rRNA5YP2KyP5w8RSdIw63goJNkO+BLwzqr6HXA58CqgB1gBXLa+6yCbb3BTkSRzkyxMsnA0X14uSWNRR0MhyUT6A+H6qvoXgKr6dVWtq6o/AFfxzC6iPmCXAZtPBx569phVdWVV9VZV75QpUzpZviSNO508+yjA1cD9VfWRAe1TB3T7a2Bxs3wTcGqSbZLsBuwO3N2p+iRJG+rk2UezgTOAHyVZ1LT9A3Bakh76dw0tA94GUFVLkswHfkz/mUvneOaRJI2sjoVCVX2XwY8TfO15trkYuLhTNUmSnp9XNEuSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKk1pFBIcvtQ2iRJY9vzPk8hySTgT4HJSV7CM89HeBHw8g7XJkkaYRt7yM7bgHfSHwD38Ewo/A745w7WJUnqgucNhar6OPDxJOdW1SdHqCZJUpcM6XGcVfXJJH8BzBi4TVXN61BdkqQuGOqB5uuADwOHAAc2r96NbLNLkm8nuT/JkiTnNe07JrktyQPN+0ua9iT5RJKlSe5Lsv8W/TJJ0iYb0kyB/gDYs6pqE8ZeC/xtVf0gyfbAPUluA84Gbq+qS5O8G3g38PfAMcDuzWsWcHnzLkkaIUO9TmEx8LJNGbiqVlTVD5rl1cD9wDTgOODaptu1wPHN8nHAvOr3PWCHJFM35TslSVtmqDOFycCPk9wNPL2+sareMJSNk8wA9gPuAl5aVSua7Vck2bnpNg1YPmCzvqZtxbPGmgvMBdh1112HWL4kaSiGGgoXbO4XJNkO+BLwzqr6XZLn7DpI2wa7q6rqSuBKgN7e3k3ZnSVJ2oihnn30vzdn8CQT6Q+E66vqX5rmXyeZ2swSpgIPN+19wC4DNp8OPLQ53ytJ2jxDPftodZLfNa+nkqxL8ruNbBPgauD+qvrIgFU3AWc1y2cBXx3QfmZzFtLBwG/X72aSJI2Moc4Uth/4OcnxwEEb2Ww2cAbwoySLmrZ/AC4F5ieZA/wCOLlZ9zXgWGAp8ATw5qHUJkkaPkM9pvBHquorzemkz9fnuwx+nADgyEH6F3DO5tQjSRoeQwqFJCcM+LgV/dcteJBXkl5ghjpT+KsBy2uBZfRfVyBJegEZ6jEF9+9L0jgw1LOPpif5cpKHk/w6yZeSTO90cZKkkTXU21x8lv5TRl9O/1XG/6tpkyS9gAw1FKZU1Weram3zugaY0sG6JEldMNRQeCTJm5JMaF5vAlZ1sjBJ0sgbaii8BTgF+BX9N6g7CS8uk6QXnKGekvpB4Kyq+g30PyiH/ofuvKVThUmSRt5QZwr7rg8EgKp6lP5bYUuSXkCGGgpbrX9sJrQzhc26RYYkafQa6h/2y4D/k+SL9N/e4hTg4o5VJUnqiqFe0TwvyULgCPpvcndCVf24o5VJkkbckHcBNSFgEEjSC9hQjylIksYBQ0GS1DIUJEktQ0GS1DIUJEmtjoVCks80z19YPKDtgiS/TLKoeR07YN17kixN8pMkr+tUXZKk59bJmcI1wNGDtH+0qnqa19cAkuwJnArs1WzzqSQTOlibJGkQHQuFqvoO8OgQux8H3FhVT1fVz4ClwEGdqk2SNLhuHFN4e5L7mt1L6++nNA1YPqBPX9O2gSRzkyxMsnDlypWdrlWSxpWRDoXLgVcBPfQ/l+Gypj2D9K3BBqiqK6uqt6p6p0zx4W+SNJxGNBSq6tdVta6q/gBcxTO7iPqAXQZ0nQ48NJK1SZJGOBSSTB3w8a+B9Wcm3QScmmSbJLsBuwN3j2RtkqQOPhMhyQ3A4cDkJH3AB4DDk/TQv2toGfA2gKpakmQ+/TfcWwucU1XrOlWbJGlwHQuFqjptkOarn6f/xfiMBknqKq9oliS1DAVJUsvnLEuj1AHnz+t2CZvsnn86s9slaAs5U5AktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktbyiWdK4NvuTs7tdwiZZcO6Cjo7vTEGS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEmtjoVCks8keTjJ4gFtOya5LckDzftLmvYk+USSpUnuS7J/p+qSJD23Ts4UrgGOflbbu4Hbq2p34PbmM8AxwO7Nay5weQfrkiQ9h46FQlV9B3j0Wc3HAdc2y9cCxw9on1f9vgfskGRqp2qTJA1upI8pvLSqVgA07zs37dOA5QP69TVtG0gyN8nCJAtXrlzZ0WIlabwZLQeaM0hbDdaxqq6sqt6q6p0yZUqHy5Kk8WWk73306yRTq2pFs3vo4aa9D9hlQL/pwEMjXNuY8YuL9ul2CZvmJS/qdgWShmikZwo3AWc1y2cBXx3QfmZzFtLBwG/X72aSJI2cjs0UktwAHA5MTtIHfAC4FJifZA7wC+DkpvvXgGOBpcATwJs7VZck6bl1LBSq6rTnWHXkIH0LOKdTtUiShma0HGiWJI0CPmRH0rAZcydBgCdCPIszBUlSy1CQJLUMBUlSa9wfUzjg/HndLmGTfXn7blcg6YXKmYIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqdWVu6QmWQasBtYBa6uqN8mOwOeBGcAy4JSq+k036pOk8aqbM4X/VFU9VdXbfH43cHtV7Q7c3nyWJI2g0bT76Djg2mb5WuD4LtYiSeNSt0KhgG8muSfJ3KbtpVW1AqB533mwDZPMTbIwycKVK1eOULmSND5068lrs6vqoSQ7A7cl+fehblhVVwJXAvT29lanCpSk8agrM4Wqeqh5fxj4MnAQ8OskUwGa94e7UZskjWcjHgpJ/izJ9uuXgf8MLAZuAs5qup0FfHWka5Ok8a4bu49eCnw5yfrv/59VdWuS7wPzk8wBfgGc3IXaJGlcG/FQqKoHgZmDtK8CjhzpeiRJzxhNp6RKkrrMUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVJr1IVCkqOT/CTJ0iTv7nY9kjSejKpQSDIB+GfgGGBP4LQke3a3KkkaP0ZVKAAHAUur6sGq+j1wI3Bcl2uSpHFj624X8CzTgOUDPvcBswZ2SDIXmNt8fDzJT0aotlHjFZ0bejLwSOeGHzvyjnS7hDHJf5udN0z/Np/z/6rRFgqD/dr6ow9VVwJXjkw540uShVXV2+06pGfz3+bIGW27j/qAXQZ8ng481KVaJGncGW2h8H1g9yS7JfkT4FTgpi7XJEnjxqjafVRVa5O8HfgGMAH4TFUt6XJZ44m75TRa+W9zhKSqNt5LkjQujLbdR5KkLjIUJEktQ0HeWkSjVpLPJHk4yeJu1zJeGArjnLcW0Sh3DXB0t4sYTwwFeWsRjVpV9R3g0W7XMZ4YChrs1iLTulSLpC4zFLTRW4tIGj8MBXlrEUktQ0HeWkRSy1AY56pqLbD+1iL3A/O9tYhGiyQ3AP8G/HmSviRzul3TC523uZAktZwpSJJahoIkqWUoSJJahoIkqWUoSJJahoIEJHlvkiVJ7kuyKMmsYRjzDcN119kkjw/HONLGeEqqxr0krwE+AhxeVU8nmQz8SVVt9MruJFs313p0usbHq2q7Tn+P5ExBgqnAI1X1NEBVPVJVDyVZ1gQESXqT3NksX5DkyiTfBOYluSvJXusHS3JnkgOSnJ3kvyd5cTPWVs36P02yPMnEJK9KcmuSe5L8a5JXN312S/JvSb6f5IMj/L+HxjFDQYJvArsk+b9JPpXktUPY5gDguKp6I/23Gz8FIMlU4OVVdc/6jlX1W+CHwPpx/wr4RlWtof+B9OdW1QHA3wGfavp8HLi8qg4EfrXFv1AaIkNB415VPU7/H/m5wErg80nO3shmN1XVk83yfODkZvkU4AuD9P888DfN8qnNd2wH/AXwhSSLgP9B/6wFYDZwQ7N83Sb9IGkLbN3tAqTRoKrWAXcCdyb5EXAWsJZn/sNp0rM2+X8Dtv1lklVJ9qX/D//bBvmKm4B/TLIj/QF0B/BnwGNV1fNcZW3mz5E2mzMFjXtJ/jzJ7gOaeoCfA8vo/wMOcOJGhrkR+K/Ai6vqR89e2cxG7qZ/t9DNVbWuqn4H/CzJyU0dSTKz2WQB/TMKgNM3/VdJm8dQkGA74NokP05yH/3Pqr4AuBD4eJJ/BdZtZIwv0v9HfP7z9Pk88Kbmfb3TgTlJfggs4ZlHoZ4HnJPk+8CLN+3nSJvPU1IlSS1nCpKklqEgSWoZCpKklqEgSWoZCpKklqEgSWoZCpKk1v8H6eFHp5MROLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Survived',hue='Pclass',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there is a clear correlation here. As someone in first class, there was a ~60% chance of survival. The difference with second class is not very large, but the difference for third class passengers is very apparent. While the number of people that were in first and second class were almost equal, the number of people in third class is comparatively very large. We can see that people in third class had a very low rate of survival. \n",
    "\n",
    "Taking a look into how and when the accident happened, we can also conclude why this was the case. The Titanic hit an iceberg at 11:40 p.m. ship's time, which meant that most people were sleeping in their cabins at the time of the incident. People were initially instructed to stay in their cabins, and by the time it was evident that the ship was actually going to sink, some of the lifeboats had already been launched even though they were only partially loaded. Although Titanic had advanced safety features, such as watertight compartments and remotely activated watertight doors, it only carried enough lifeboats for 1,178 people â€” about half the number on board. Many of the third class passengers were unable to get into a lifeboat, and when the boat broke apart and sank at 2:20 a.m. there were still well over one-thousand people aboard. Just under two hours after Titanic sank, the liner RMS Carpathia arrived, which took the surviving people from the lifeboats and the icecold water aboard.\n",
    "\n",
    "### A final note\n",
    "\n",
    "When building models, it is always important to take your time to analyse the data properly. Many times, aspects of the data, such as how, when, or under what circumstances it was gathered may have influenced it greatly. Using these aspects of the data into advantage may result in a model that is more accurate. Below, we have included a final boxplot that shows one of the improvements that can possibly be made to our data cleaning. We have inferred the missing data in the Age column by taking the average age of all passengers, but it might be better to infer the age of a passenger by taking the passenger class averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x177bee47a08>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x='Pclass',y='Age',data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
