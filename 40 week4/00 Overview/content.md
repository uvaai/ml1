
# Week 4

Up to this point we've only covered *classification* algorithms. This week
we’ll get a first introduction into *regression*, the other main type of
supervised learning. Regression is all about fitting lines and predicting real
numbered values instead of classes.

For this, we’ll use the classic machine learning technique of defining a cost
function and trying to find the model that generates predictions with minimal
cost.  In order to minimize this function, we’ll need to be able to compute the
gradient of the function, which is what this week's [SOWISO module](/week4/gradients) 
is all about.

The theory of regression itself will be covered in more detail in the
[video lectures by Andrew Ng](/week4/linear-regression) on the topic.
In the notebook this week you’ll work on implementing
[univariate linear regression](/week4/gradient-descent), which is the most
basic form of regression, but already introduces a lot of the relevant concepts
we’ll need for more complex techniques later on.

For the written assignment this week, we'll look ahead to what those
more complex techniques [can achieve in practice](/week4/alphago) and what that
means for the possibilities of such systems.
