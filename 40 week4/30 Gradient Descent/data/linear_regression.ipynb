{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77d3829cfbda19be09d1be8cb50050b5",
     "grade": false,
     "grade_id": "cell-0d93d316e8abb0ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Univariate Linear Regression\n",
    "\n",
    "For the programming notebook this week we'll build all the components for a univariate linear regression model. It is a very simple regression model, but many of the ideas here are a part of more complex regression models too, so understanding them in this simple context will be good practice.\n",
    "\n",
    "To start, let's load some of the tools and data we'll need. *Matplotlib* will allow us to graphically plot the data and the results, making everything a little easier to interpret. *Numpy* is a library that is used a lot in machine learning when building models, but here we'll only use the function `loadtxt` as an easy way to load in the data file. The function `train_test_split` from *scikit-learn* will once again be used to split the data into *70%* training and *30%* testing.\n",
    "\n",
    "*Note: If you're getting a `FileNotFoundError` while running this cell, make sure you downloaded data file correctly and have put that file in the same folder as your notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d954cbeee93166735f940ec4f1fd357",
     "grade": false,
     "grade_id": "cell-98cf1d56ea1ac4fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from notebook_checker import start_checks\n",
    "# Start automatic globals checks\n",
    "%start_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec7778ca2331434a4befcedf68a15011",
     "grade": false,
     "grade_id": "cell-b1ca4f0278c51ca3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tell matplotlib that the plots should be shown inside the notebook as an image\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data file as floats\n",
    "data = np.loadtxt('linear_data.csv', delimiter=',', dtype=float)\n",
    "\n",
    "# Split the data into 70% training and 30% testing\n",
    "training_data, testing_data = train_test_split(data, train_size=0.7)\n",
    "\n",
    "print(\"First 10 rows of the training data:\\n\")\n",
    "print(training_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3c438330f034b38efb44a3c7bc1a89a",
     "grade": false,
     "grade_id": "cell-449809e7af772aad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Numpy Arrays\n",
    "\n",
    "The training set is now stored in a *Numpy* data matrix, with every row corresponding to a single training example of one x-value and one y-value. A data matrix is very similar to a *list of lists*, but the matrix format has some advantages, which we'll get into in later assignments. For now, we're just going to convert this data back to two simple lists (technically *Numpy* arrays, but you can use them exactly like you would a list).\n",
    "\n",
    "This is a simple artificial data set that was generated to be suitable for linear regression, but to make it more concrete you could still view this as the housing prediction problem from the theory videos. The x-values would then be square meters and the y-values the price of the sold house in thousands of euros. This data would then be a set of records of past sales and we're trying to predict the price of new houses based on just the number of square meters they each contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48c40b4f7bbe8c22f8b122e3cbe68052",
     "grade": false,
     "grade_id": "cell-95f3847d4ec34268",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Separate the training data into x and y arrays\n",
    "x_training = training_data[:, 0]\n",
    "y_training = training_data[:, 1]\n",
    "\n",
    "# Separate the testing data into x and y arrays\n",
    "x_testing = testing_data[:, 0]\n",
    "y_testing = testing_data[:, 1]\n",
    "\n",
    "print(\"\\nTraining x-values\")\n",
    "print(x_training)\n",
    "\n",
    "print(\"\\nTraining y-values\")\n",
    "print(y_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd1fabf5ca9b0f2f48d429d7117b970e",
     "grade": false,
     "grade_id": "cell-23806e5cc7e78436",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 1: Plotting the data\n",
    "\n",
    "The left column of the training data matrix is now converted to a list of x-values and the right column of the training data matrix is now converted to a list of y-values. It is important to note that, even though we split the data into two lists, the indices between these two lists are linked. So, for example, the x-value at index 5 and the y-value index 5 together form a single training pair.\n",
    "\n",
    "These pairs of values, at the same index in the x-values and the y-values lists, together form a sample of data for the sale of a single house, with its surface area and price. Viewing them as a plotted point instead of lists of numbers, would therefore probably be more informative. Use the *matplotlib* functions [plt.plot()](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.plot.html) and [plt.show()](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.show.html) to create a plot for the training data. Make sure to plot **points** and not a line, as these are individual (shuffled) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5b58754b5b4db20ccff18fa8c27e9ca",
     "grade": true,
     "grade_id": "cell-322410bd07c439ec",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3982d28a694efa240bc2d09e59ba0ded",
     "grade": false,
     "grade_id": "cell-24bbdf059e5f47c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2a: Linear model\n",
    "\n",
    "Next, we'll define the simple linear model we'll use to try and predict the y-values. A linear equation is defined as an equation of the form:\n",
    "\n",
    "$$y=ax+b$$\n",
    "\n",
    "*Note:* In the theory videos Andrew uses $\\theta_0$ and $\\theta_1$ as the model parameters, but we'll stick with $a$ and $b$ for now, just to make sure the distinction between the two is very clear. The underlying model is of course identical, and we'll still use a $\\theta$ parameter notation in later assignments.\n",
    "\n",
    "Start by implementing the function `linear_model()`, which takes an input `x` and the model parameters `a` and `b` and returns the prediction for `y` (this should be very straightforward). Next, make a function `linear_model_list()` which takes a list of x-values and applies the `linear_model()` function to every x-value in the list and returns a new list of predicted y-values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00b36ed3f4c265c5380ecc8f4fd08106",
     "grade": true,
     "grade_id": "cell-e423bfa8371a05b0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, a, b):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "def linear_model_list(x_list, a, b):\n",
    "    # YOUR CODE HERE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "205fd67c08086c792cbf72aa8243473d",
     "grade": false,
     "grade_id": "cell-df75ec91c3bd4676",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2b: Plotting a model\n",
    "\n",
    "Now, let's plot to see what such a model would look like. This model is a linear function and so by definition it can make predictions for *any* possible x-value. If we want to plot this line, the best thing we can do is just sample a lot of points on the x-axis, compute the corresponding y-values for each of them and plot all of these points, but connect every point together with a straight line.\n",
    "\n",
    "*Side note: This might seem like a strange way to plot a linear function, but this is actually how any function is plotted on a computer. This also includes curved functions, like for example parabola or sine waves, so it will be good practice to use the same approach here. Just take a whole lot of points and compute the output value for each of them and let the computer \"connect the dots\".*\n",
    "\n",
    "Based on the plot you made before, make an approximate guess for what you think $a$ and $b$ should be. Make a range of x-values using [np.linspace()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) over a sensible interval (again, take a look at the plot from the previous cell for some hints). Compute the predicted y-values for those sampled x-values and your guesses for $a$ and $b$ using `linear_model_list()`.\n",
    "\n",
    "Plot the training data as points, exactly as you did before, but don't use `plt.show()` yet. Next, plot the sampled x-values and the predicted y-values as a **line** using a different color (see the [plot documentation](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.plot.html) for details). Finally, show the results. You should end up with a plot that has both your predictive line, and the training data.\n",
    "\n",
    "Try at least 3 different values for $a$ and $b$ in your plot and see how well the predicted line fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "905591611ea3e9e323bc26419a44e0f5",
     "grade": true,
     "grade_id": "cell-e57220ca29a701b7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "964111e16ad170c60d19c317900da282",
     "grade": false,
     "grade_id": "cell-aa084ab1c76fcb21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q1. Which values for $a$ and $b$ did you try and which one seemed to fit the data the best?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ba41964daabfaaf4618c606f3887118",
     "grade": true,
     "grade_id": "cell-565e287215079d2b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76bd832525c4ac60a001bb7813b5f406",
     "grade": false,
     "grade_id": "cell-ac33787758dd7099",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 3: Cost function\n",
    "\n",
    "Now, we'll define the cost function for this linear model:\n",
    "\n",
    "$$J(a, b) = \\frac{1}{2m}\\sum^m_{i=1} ((a x^i + b) - y^i)^2$$\n",
    "\n",
    "Note that this function only depends on the model parameters $a$ and $b$, and not the data vectors $\\mathbf{x}$ and $\\mathbf{y}$. This training data is considered to be *constant* within the cost function, as the data will not change at all. The only thing that changes the cost of the model for a specific problem is changing the model parameters $a$ and $b$.\n",
    "\n",
    "In order to make a proper function out of the input, this function *does* of course also depend on the x-values and y-values of the training data. Define the function `linear_cost`, which takes model parameters `a` and `b`, and a data set in the form of lists of `x_values` and `y_values`, and computes the model cost for that data set. Reuse your functions from before as much as possible.\n",
    "\n",
    "Apply this function to your guesses for $a$ and $b$ based on the plots of the training data before. Print the parameters and resulting cost for each guess. Order the guesses from highest cost to lowest cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b65c886dece255a3377e96451ef0083f",
     "grade": true,
     "grade_id": "cell-305dc0f4dc22e2df",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_cost(a, b, x_values, y_values):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fc33a1e041edf638863d56569d117c1",
     "grade": false,
     "grade_id": "cell-a0a91daa4d1e30ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2. Did the ordering of the costs for each guess correspond with your expectations? Explain your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af9f843ea1fb15ba44bf7dbd7a53bb69",
     "grade": true,
     "grade_id": "cell-3f4d96d52e92907d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76f10c14679612842fbdaf2158cb2d52",
     "grade": false,
     "grade_id": "cell-86fc09cb40b22b56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3D plot of the cost surface\n",
    "\n",
    "Now, let's take a further look at this cost function and construct a full 3D plot. Below is the code to plot the complete cost surface. The code takes 100 samples for possible values of $a$ and 100 samples for possible values of $b$ and then computes the cost for *every possible combination*. Here you can start to get a hint of why *Numpy* is used so often in machine learning, as we can do this complex operation in only a couple of lines of code.\n",
    "\n",
    "The code then simply plots these computed results and connects the dots in the same way as for the line, but now forming a 3-dimensional surface. The x-axis and y-axis are the values for $a$ and $b$ respective, with the z-axis containing the corresponding cost for that combination of $a$ and $b$. The scale for $a$ is from 0 to 5 and for $b$ is from -100 to 200, as this is my own estimate of the range of sensible value combinations that will *definitely* contain the minimum of the cost function.\n",
    "\n",
    "Make sure you understand this 3-d plot and how it relates to the line and point plots from earlier, before moving on. Understanding how this surface relates to the estimates made by the model will be critical for the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8c9b0d4197354fc33b2f792deab42c2",
     "grade": false,
     "grade_id": "cell-c24c44fc7409845b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Make a large 3D plotting figure\n",
    "plt.rc('axes', labelsize=24)\n",
    "ax = plt.figure(figsize=(16,12)).add_subplot(111, projection='3d')\n",
    "\n",
    "# Create evenly spaced samples for a and b\n",
    "a = np.linspace(0, 5, 100)\n",
    "b = np.linspace(-100, 200, 100)\n",
    "\n",
    "# Create the 3D plot data (no need to understand this part yet)\n",
    "X, Y = np.meshgrid(a, b)\n",
    "cost_map = np.vectorize(linear_cost, excluded=(2, 3))\n",
    "Z = cost_map(np.ravel(X), np.ravel(Y), x_training, y_training).reshape(X.shape)\n",
    "\n",
    "# Do the actual plotting and labeling of the axis\n",
    "ax.plot_wireframe(X, Y, Z)\n",
    "ax.set_xlabel('a')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b95d5a37e312a6d8e8003c3209f7e64b",
     "grade": false,
     "grade_id": "cell-6d11ee3522d10748",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Finding the minimum of the cost function\n",
    "\n",
    "The goal in linear regression is to try and find the model parameters $a$ and $b$ that result in the lowest possible cost on the training data, i.e. the lowest point on the surface plotted above. Given that we've just computed the cost for a lot of different combinations of $a$ and $b$, we could just select the combination with the lowest cost from this plot. This approach has two problems:\n",
    "\n",
    "* This will really only give us quite a coarse approximation, as we've taken 100 samples for both $a$ and $b$, so the space between samples is large and ideally we'd have a more precise solution.\n",
    "* It usually isn't feasible to compute all combinations in this manner (which is part of the reason why we're using this simple generated data set) and this approach won't work for larger data sets.\n",
    "\n",
    "So instead we'll use a *gradient descent* approach, where we start with a random value for $a$ and $b$ and iteratively follow the gradient down to the minimum value. For this we'll need to be able to compute what the gradient of the cost function is at a specific point $a, b$. If we can obtain the partial derivative of the cost function with respect to $a$ and $b$, then we can easily compute the gradient at any point.\n",
    "\n",
    "### LaTeX\n",
    "\n",
    "The next few assignments in this notebook will require you to write some equations in *Markdown* cells. There is a language called *LaTeX* that is used in almost all scientific disciplines to write equations, which you can write directly in *Markdown* cell. We'll only need a couple of simple tools, listed below:\n",
    "\n",
    "* All equations should be surrounded by `$$` on both sides\n",
    "* `\\frac{1}{2}` makes a fraction of 1 over 2: $\\frac{1}{2}$\n",
    "* `\\sum` makes a sum symbol: $\\sum$\n",
    "* `^` makes next character superscript\n",
    "* `_` makes next character subscript\n",
    "* `\\partial` makes the partial derivative symbol: $\\partial$\n",
    "\n",
    "If you *run* a markdown cell you can see the rendered equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4a: Partial derivative of $J(a, b)$ w.r.t. $b$\n",
    "\n",
    "For this assignment you should work out what the partial derivative of the cost function is with respect to $b$. This will already give us one half of the gradient of the cost function, and it is a slightly easier derivative to start with.\n",
    "\n",
    "Apply the rules of derivation to the equation below and simplify the result as much as possible. You should label every step with the name of the rule you use to determine the next equation. Continue to apply rules until you can no longer simplify. The first step has already been given as an example:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial b} J(a, b)$$\n",
    "\n",
    "Substitute the definition\n",
    "\n",
    "$$\\frac{\\partial}{\\partial b} \\frac{1}{2m}\\sum^m_{i=1} ((a x^i + b) - y^i)^2$$\n",
    "\n",
    "***Continue with the next rule here***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb591adc8e3ff07c76f8758af5a847bd",
     "grade": false,
     "grade_id": "cell-640cec1874d10270",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 4b: First half of the gradient\n",
    "\n",
    "With this equation you should now be able to compute the $b$ component of the gradient of the cost function at a specific point $a, b$. Fill in the function `b_gradient()` to compute the equation you obtained for this at the previous step. We will check the correctness of the function (and the equation) at later steps, so for now implement the function and continue with the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82990b9851e551abfc781566a194d697",
     "grade": true,
     "grade_id": "cell-8e46a8aaaf4b4826",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def b_gradient(a, b, x_values, y_values):\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 5a: Partial derivative of $J(a, b)$ w.r.t. $a$\n",
    "\n",
    "For this assignment you should work out what the partial derivative of the cost function is with respect to $a$. It will be very similar to steps you just completed for $b$, and together they will define the complete gradient of the cost function. \n",
    "\n",
    "Apply the rules of derivation to the equation below and simplify the result as much as possible. You should label every step with the name of the rule you use to determine the next equation. Continue to apply rules until you can no longer simplify. The first step has already been given as an example:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial a} J(a, b)$$\n",
    "\n",
    "Substitute the definition\n",
    "\n",
    "$$\\frac{\\partial}{\\partial a} \\frac{1}{2m}\\sum^m_{i=1} ((a x^i + b) - y^i)^2$$\n",
    "\n",
    "***Continue with the next rule here***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f234e50d4de73b3547464093b9cd75a",
     "grade": false,
     "grade_id": "cell-bb883045e6ebf6b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 5b: Second half of the gradient\n",
    "\n",
    "With this equation you should now be able to compute the $a$ component of the gradient of the cost function at a specific point $a, b$. Fill in the function `a_gradient()` to compute the equation you obtained for this at the previous step. We will check the correctness of the function (and the equation) at later steps, so for now implement the function and continue with the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1a1aae9c01c2f5a105edfce9adb0eb3",
     "grade": true,
     "grade_id": "cell-8e167c76f1e3a524",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def a_gradient(a, b, x_values, y_values):\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07bf49b1ffd0041867b74d42b75f7e0c",
     "grade": false,
     "grade_id": "cell-36ada27d77928c35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6: Approximating the gradient\n",
    "\n",
    "Instead of this exact computation, we can also just make a numeric approximation of the gradient. For this we can use the *difference quotient*, which defines the *rate of change* of a function $f$ at a specific value of $x$:\n",
    "\n",
    "$$\\frac{f(x + \\epsilon) - f(x)}{\\epsilon}$$\n",
    "\n",
    "If $\\epsilon$ is small enough, then this difference quotient should approximate the derivative of the function $f$ at $x$. We can use this same approximation for the partial derivative of the cost function with respect to $a$ and $b$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial a} J(a, b) \\approx \\frac{J(a + \\epsilon,\\ b) - J(a, b)}{\\epsilon}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial b} J(a, b) \\approx \\frac{J(a,\\ b + \\epsilon) - J(a, b)}{\\epsilon}$$\n",
    "\n",
    "We can use this numerical approximation to check the analytical gradient; if they both come out to around the same value, then it is much more likely that the analytical gradient was correct (which can be especially difficult to ensure for larger models). We can then use the much more exact, and faster, analytical gradient when optimizing the model, while having some certainty it is correct.\n",
    "\n",
    "Implement the functions `a_gradient_approx()` and `b_gradient_approx()`, which should approximate the partial derivative of the cost function w.r.t $a$ and $b$ respectively, for a set of `x_values` and `y_values`, at the point $a,b$. The default value for epsilon is set to $\\epsilon = 10^{-6}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb4a7f61ee867b7f42e0e42743029ebb",
     "grade": true,
     "grade_id": "cell-fba1b2e789051430",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def a_gradient_approx(a, b, x_values, y_values, eps=10**-6):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "def b_gradient_approx(a, b, x_values, y_values, eps=10**-6):\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1efd3a3a77e7bcafa8104cf7b83ac257",
     "grade": false,
     "grade_id": "cell-49517cf92ac7c7c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 7: Checking the gradient\n",
    "\n",
    "Now that we have functions for both the analytical and the numerical gradient, let's write some code to compare them. First, write the function `check_gradient()`, which compares the results of `a_gradient()` with `a_gradient_approx()`, and the results of `b_gradient()` with `b_gradient_approx()`, for the cost function $J$ at a specific point $a,b$. The function should check the absolute difference between each approximation and its analytical counterpart, for which you can use the built-in function [abs](https://docs.python.org/3/library/functions.html#abs). If this absolute difference is greater than some threshold value `thres` for either half of the gradient, the function should print out the approximation error and return `False`. If both approximations were within the threshold range, the function should return `True`.\n",
    "\n",
    "The function described above will only check the gradient at a specific point $a,b$. So next, write the function `check_gradient_loop()`, that will try `iterations` number of different random points $a,b$, and check the gradient for each of them. Use the built-in [random.uniform](https://docs.python.org/3/library/random.html#random.uniform) distribution to generate random values for $a$ between `a_min` and `a_max`, and for $b$ between `b_min` and `b_max`. Your function should then use `check_gradient()` to compare the analytical and approximate gradient at that random point. The function `check_gradient()` will already print an error message if the gradient approximations differ by more than `thres`, but if each of the `iterations` number of random checks pass without an error, the function `check_gradient_loop()` should print a message saying that all gradients seem correct.\n",
    "\n",
    "Call the `check_gradient_loop()` function at the end of the cell using the training data and some reasonable ranges for $a$ and $b$. Make sure the analytical gradients all seem correct before moving to the next assignment.\n",
    "\n",
    "*Note:* The data points on which you're evaluating, i.e. the `x_values` and `y_values`, are considered constant when determining the cost, as only the model parameters $a$ and $b$ can affect how good the model predictions are and thus change the cost. However, the data is still essential when determining the cost, as we can't measure how good the fit of the model is without data to compare the prediction to. The same holds for the gradient, where the data points in `x_values` and `y_values` are constant, but still necessary to determine the gradient, as they determine the shape of the cost function and thus, by definition, the gradient. This why `x_values` and `y_values` must be passed as arguments for all these functions, even if their value remains constant. Using `x_training` and `y_training` when calling `check_gradient_loop()` means that we check the gradients with the same data that we will use to actually calculate the gradients in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9616328a9c3eac4bd5263601f8238b76",
     "grade": true,
     "grade_id": "cell-065460b2f97803c5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def check_gradient(a, b, x_values, y_values, thres):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "def check_gradient_loop(a_min, a_max, b_min, b_max, x_values, y_values, thres, iterations=10**5):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9fd275a67aa6d208e13793127072ab5",
     "grade": false,
     "grade_id": "cell-efc4265af4b7cfc75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It can actually be very hard to verify *with certainty* that gradient functions are correct, as that would require knowing exactly what the gradients values should be be for some combination of data set and parameter values. In practice, there isn't really a feasible way to test this *exactly*, especially as the models become larger and more complex. By comparing the gradient to an approximation based on the cost function, we can still try and convince ourselves the gradient functions are likely correct. If many repeated comparisions for different parameter values all result a gradient close to the approximation, then the functions are probably correct.\n",
    "\n",
    "Note that if you make the value of `thres` large enough, that all checks will pass, even if the gradient is actually incorrect. Conversely, for a small enough value of `thres` some tests will also always fail, as accuracy of the gradient approximation necessarily depends on the size of $\\epsilon$. Start with larger values of `thres` and systematically decrease it until you get an approximation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e534a1d459d91c52769130d68e665a5b",
     "grade": false,
     "grade_id": "cell-bc3dc0b4976b44af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q3. At what value of `thres` did you start to get an approximation error? What were the actual gradient and approximated gradient that gave you the error, and how much did they differ relatively (as a percentage)? Comparing this to the size of $\\epsilon$ used for the approximations, do you think it *likely* your implementations for `a_gradient` and `b_gradient` are correct?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de13fc1bf852ec7005da77fe7e002bb0",
     "grade": true,
     "grade_id": "cell-bb0adfa0586864f1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3a7b3f2419972124ce1bb5fc5a6d220",
     "grade": false,
     "grade_id": "cell-22ce3e4ae414df50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "If you're convinced that the gradient functions are correct, we can start to actually build *gradient descent*. Obtaining the correct gradient for the cost function can be the tricky part of this algorithm, but with that done, that descent part of the algorithm should be pretty straightforward.\n",
    "\n",
    "All you really need to do is pick a starting point $a,b$ and repeatedly compute the gradient and move in the *opposite* direction, i.e. the gradient will \"point up\" and you want to move down. Then you just need a learning rate parameter $\\alpha$ to control how big each step down is, and another threshold value to determine if your algorithm has *converged* and you've reached the minimum.\n",
    "\n",
    "One step of gradient descent for $a$ then just becomes\n",
    "\n",
    "$$a = a - \\alpha \\frac{\\partial J(a,b)}{\\partial a}$$\n",
    "\n",
    "And for $b$\n",
    "\n",
    "$$b = b - \\alpha \\frac{\\partial J(a,b)}{\\partial b}$$\n",
    "\n",
    "The *minus* sign ensures the move is in the opposite direction, and $\\alpha$ scales the size of each gradient step. Note that these gradient terms should both be computed *before* you actually update the parameters $a$ and $b$, as you want the changes to happen *simultaneously*.\n",
    "\n",
    "Because this cost function is actually *convex*, it really doesn't matter where we start with $a$ and $b$, as all points should eventually lead to the same *global minimum*. We'll get back to what this means exactly, and in what cases you don't have these nice guarantees, in later assignments. For now, it is good to know that the starting point shouldn't matter. We could have the algorithm randomly start in some range, as with `gradient_check_loop()`, but then we'd need minimum and maximum values for both parameters, so the easy solution for now is just to have them both start at 0.\n",
    "\n",
    "Now we already have all the elements to do one step of gradient descent. The only remaining element is really to just repeat this in a loop and compute the cost at every step. If we know the cost of the current step and the previous step, then we can compute the difference between the two, and see how much progress the algorithm made that step. This gives us two important pieces of information:\n",
    "\n",
    "1. If the difference in cost becomes small enough, then we know the algorithm has *converged* and we can stop the loop.\n",
    "2. If the difference in cost ever becomes *negative*, then the cost is increasing, and not decreasing. This means the algorithm is *diverging* and the step size of the learning rate must be too large.\n",
    "\n",
    "If your code ever encounters situation *2*, the function should print an error message to inform the user. For situation *1* we'll also use a threshold value to determine if the difference is small enough, the default value for which is now set to $10^{-6}$. Note that you could also include using the function `check_gradient()` at every step of gradient descent, but this would be quite a bit slower. Also, as the function above takes a large random sample of the gradients, we can be pretty confident that this part should work correctly. You may add a call to `check_gradient()` in your loop, but this is not required.\n",
    "\n",
    "### Assignment 8: Implementing Gradient Descent\n",
    "\n",
    "Write the function `gradient_descent()` according to the description above. The function should return the converged parameters $a,b$ when completed.\n",
    "\n",
    "When your function is done, call the function using the training data to see what your computed best estimates for $a,b$ are. Print the $a$ and $b$ you found, and then print the cost on the training data and finally, also print the cost on the testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb80e1c5562c291f1eec6b3cb145786a",
     "grade": true,
     "grade_id": "cell-75fdede3b831c147",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x_values, y_values, alpha, a_start=0, b_start=0, thres=10**-6):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return (a, b)\n",
    "        \n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b3bf138f50fb642e9fb6a15bb4f0627",
     "grade": false,
     "grade_id": "cell-a84fd1e460bcba8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It should be the case that for a large enough value of $\\alpha$, the cost will always *diverge* at some point. Conversely, if $\\alpha$ is very small, then the algorithm will still *converge*, but the large number of steps required means that this will take a long time. So, a good strategy is to start with a value for $\\alpha$ that does *diverge* and then systematically decrease it until you get a value that *converges*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d20cfe847c0462642fcb45bb3921ba0e",
     "grade": false,
     "grade_id": "cell-4647ad856235f722",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q4. Give 3 different values for alpha: One that causes the algorithm to diverge, one that results in a reasonable converged solution, and one that is very slow to converge (> 5 minutes running time).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7bc020d58b0d4b09086d990452be480",
     "grade": true,
     "grade_id": "cell-f5e53550d02b5e76",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6476f826d3f4cdbcbcaaaf60a486cfe",
     "grade": false,
     "grade_id": "cell-7c237782bec9178d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q5. What is the best value of alpha you found? Explain why you think this is the best learning rate to use for this data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "411efee9a08912057bad51ea96bb68a6",
     "grade": true,
     "grade_id": "cell-5496d06341cc0ec3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59b21291632e39e29ef81a4bab224db6",
     "grade": false,
     "grade_id": "cell-a31b9ee9f7c6ba44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 9: Plotting the best fit\n",
    "\n",
    "Copy your code to plot the training data and the estimated line from before. This time, also add the testing data to the plot, making sure to use a different color point for training and testing, so you can distinguish the two.\n",
    "\n",
    "Then, use the values your `gradient_descent()` found for $a$ and $b$ and use the function `linear_model_list()` to show the complete prediction line for that combination of $a$ and $b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ca97a5aa0dd4978fb4f65480a574c8f",
     "grade": true,
     "grade_id": "cell-a0b3a7055f2e56e0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4466aa3709a95704faff559a28baceb8",
     "grade": false,
     "grade_id": "cell-33c41167b85ad5b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q6. Based on the plot above, do you think this prediction problem is well suited to a linear model? Does this match what you expect based on the results you got for the cost for both the training and testing data? Explain your answers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6adbc406723ed7f17c98a0d262f90658",
     "grade": true,
     "grade_id": "cell-46f7d772fb208c73",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
