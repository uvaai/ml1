{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are: \n",
    "- Email; spam or not spam \n",
    "- Online transactions; fraud or not fraud\n",
    "- Tumor; malignant or benign \n",
    "\n",
    "Logistic regression transforms its output using the logistic sigmoid function to return a probability value. This probability value can then be used to decide which of the classes the input belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigmoid function\n",
    "\n",
    "To generate probabilities, logistic regression uses a function that gives outputs between 0 and 1 for all inputs. There are many functions that meet this description, but the one used in this case is the logistic function. From here on out we will refer to it as the sigmoid function:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Implement the function `sigmoid_func` that can either take a single value `X`, or an array of values `X`, and compute the *Sigmoid* function for each.\n",
    "\n",
    "*Hint: Numpy built-in functions and basic arithmetic operations work on both Numpy arrays and single values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid_func(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "assert sigmoid_func(0) == 0.5\n",
    "assert sigmoid_func(1) == np.e / (1 + np.e)\n",
    "assert np.allclose(sigmoid_func(np.array([0,1])), np.array([0.5, np.e/ (1 + np.e)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis function\n",
    "\n",
    "Last week, when using polynomial regression, we used a formula for the hypothesis:\n",
    "\n",
    "$$h(x^t) = w_0 + \\sum_{i=1}^{k} w_ix_i^t$$\n",
    "\n",
    "Which we transformed to the shape:\n",
    "\n",
    "$$ h(x^t) = \\sum_{i=0}^k D^t_iw_i $$\n",
    "\n",
    "Using the matrix $D$ for a $k^{th}$-order polynomial:\n",
    "\n",
    "$$ D = \\left[\\begin{array}{cccc}\n",
    "1 & x^1 & (x^1)^2 & \\cdots & (x^1)^k \\\\ \n",
    "1 & x^2 & (x^2)^2 & \\cdots & (x^2)^k \\\\ \n",
    "\\vdots \\\\\n",
    "1 & x^N & (x^N)^2 & \\cdots & (x^N)^k \\\\ \n",
    "\\end{array} \\right]$$\n",
    "\n",
    "For logistic regression, the formula is:\n",
    "\n",
    "$$h(x^t) = \\sigma(w_0 + \\sum_{i=1}^{k} w_ix_i^t)$$\n",
    "\n",
    "This model can compute every hypothesis value for a some matrix of $X$ values, however, as you might have noticed in the *Logistic Regression* video, there is an alternative. Just like with the *Polynomial Regression* $D$ matrix; if we expand the matrix $X$ with a column that holds $1$ values used for the bias inputs by redefining $x = [1, x^T]^T = [1, x_1, x_2, \\ldots, x_d]^T$ (superscripts are indices here) and $w = [w_0, w_1, w_2, \\ldots, w_d]^T$ then the equation simplifies to:\n",
    "\n",
    "$$h(x^t) = \\sigma(-w^T x^t)$$\n",
    "\n",
    "And the model for a single perceptron node (assuming 3 input variables) now becomes:\n",
    "\n",
    " <img src=\"NN_percep.png\" width=\"60%\"> \n",
    "\n",
    "Note that given some vector of weights $w$ and a matrix of inputs $X$, computing $w^T x^t$ for **all** of the $X$ rows just becomes a single matrix multiplication. Given that your `sigmoid_func` can also handle array inputs, transforming this array into an array of *Sigmoid* function outputs should also just be a single function call. \n",
    "\n",
    "Write the function `add_bias`, which takes a matrix of `X` values, and returns that same matrix with a column of ones added to the front. Take a look at [`np.concatenate`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html).\n",
    "\n",
    "Then write the function `logistic_model`, which takes a matrix of `X` values, a weight vector `W`, and calculates $h(X)$. Make sure to first add the bias, or the shapes of the matrix `X` and vector `W` won't match up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "def add_bias(X):\n",
    "    return np.concatenate((np.ones((X.shape[0],1)), X), axis=1)\n",
    "\n",
    "def logistic_model(X, W):\n",
    "    X = add_bias(X)\n",
    "    return sigmoid_func(X.dot(W.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
