{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Predicting shipwreck survival\n",
    "\n",
    "In this week's assignment you will implement a classification algorithm named Logistic Regression. This sounds very counter-intuitive, as regression is something entirely different than classification, but the Logistic Regression model is actually estimating (\"regressing\") the probability (which is a continuous value) that data can be assigned to a specific class out of a set of classes. We will focus on so-called binary classification problems, wherein there are two possible classes. Some of the examples of binary classification problems that Logistic Regression is able to solve are: \n",
    "\n",
    "- Email; spam or not spam \n",
    "- Online transactions; fraud or not fraud\n",
    "- Tumor; malignant or benign \n",
    "\n",
    "In this notebook, we will be using the classic *Titanic* dataset. This data consists of demographic and traveling information for 891 of the Titanic's passengers, and the ultimate goal is to predict which of these passengers survived. Here is a summary of the data set's attributes:\n",
    "- PassengerId: passenger ID assigned in this dataset\n",
    "- Survived: A Boolean indicating whether the passenger survived or not (0 = No; 1 = Yes); this is our target\n",
    "- Pclass: Passenger class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "- Name: field containing the name and title of the passenger\n",
    "- Sex: male/female\n",
    "- Age: age of the passenger in years\n",
    "- SibSp: number of siblings/spouses aboard\n",
    "- Parch: number of parents/children aboard\n",
    "- Ticket: ticket number\n",
    "- Fare: passenger fare in British Pounds\n",
    "- Cabin: location of the cabin, consisting of a letter indicating the deck, and a cabin number.\n",
    "- Embarked: port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "As you can probably see, the dataset contains a mix of textual, continuous, categorical, and Boolean variables. Before we can get into implementing and applying Logistic Regression, we will have to clean up the data. We will lead you through this process using the tools a data-scientist might use, providing you with the tools that will help you quickly prepare your own data in the future.\n",
    "\n",
    "In the code below, we have loaded the data and show its first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('titanic.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick view\n",
    "\n",
    "From the first five rows we have displayed in the table above, you might be able to see that there are multiple types of variables: we have integers, strings, floats, and a few \"NaN\"s, which are missing values. Lets take a quick look to see what the types of data that we are dealing with actually are. A tool a data-scientist might use here is Panda's [`df.info()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html), which is a method that can be applied to any `DataFrame` that shows information about a DataFrame including the index, datatype for each column, non-null values, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have 891 entries, numbered 0 to 890, with 12 different variables each. Of these 12 variables \"Survived\" will be our target variable. Out of 12 columns, 2 are floats, 5 are integers, and 5 are \"objects\" which in this case means that they are strings. Out of all of the columns, there are 3 that have some missing values. We will have to fix these missing values later.\n",
    "\n",
    "Let's take a better look at what the columns that have integers and floats in them look like. For this, we will use Panda's [`df.describe`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html?highlight=describe#pandas.DataFrame.describe) which is a method that can be applied to any `DataFrame` that shows descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a table showing a bunch of statistics for each of the columns. The `count` is the number of values that is not NaN in that specific column. We can also see mean, standard deviation, minimum, maximum, and percentiles for each of the columns. These [percentiles](https://en.wikipedia.org/wiki/Percentile) indicate the value below which a given percentage of the data in the column falls. Let's see what we can conclude from this table and the previous one:\n",
    "\n",
    "- PassengerId is a column that runs from 1 to 891, indicating the index of a passenger in this dataset. This variable has no intrinsic value and might even make our machine learning model find correlations that are no indication of the truth, so we can delete the whole column later.\n",
    "- The mean of Survived indicates that out of the 891 people in our dataset, only ~38% of people survived. This can be deduced from the mean of the data in that column, as the values are either zero or 1. As an example, when 90% of these values would be 1, we would end up with a mean of 0.9, as the other values are all zeros.\n",
    "- There are more people in Passenger Class 3 than in class 1 and 2 combined. This can be concluded from the fact that the from the 50th percentile on every passenger is in third class.\n",
    "- There is at least one baby in our dataset, with an Age of 0.42, or 5 months. The oldest person is 80 years old. We can see this from the min and max values of the Age column.\n",
    "- There are 277 people of which we do not know the Age. We have 891 data entries, but the \"count\" for age is 714. $891 - 714 = 277$ \n",
    "- The majority of people has no siblings/spouses aboard. This can be concluded from the percentiles.\n",
    "- The majority of people has no children/parents aboard. This can be concluded from the percentiles.\n",
    "- There is a big difference in scale in data. Fare can be up to 512 pounds while the number of children/parents on board is only up to 6.  We can also see this in the standard deviations, where the difference is also very large. In his videos, Andrew Ng talks about feature scaling, which is something we must apply here. We will discuss this later.\n",
    "\n",
    "Often, these tables can tell you a lot about the data you are working with, and might provide you with intuitions on what would work best for this dataset. We will spend some more time on analysing the data at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "Let's create a copy of the dataset that does not contain data that we do not need for Logistic Regression. In this case, we will delete the rows containing the Passenger ID, Name, Cabin Number, or Ticket number of each passenger. In a more advanced data processing pipeline we might opt to include this data, but that would include data mining techniques that are beyond the scope of this notebook. Keep in mind that the cabin number, for example, may include information on where the passenger was on the ship, which in turn might give information on if a passenger would have been able to reach a lifeboat timely.\n",
    "\n",
    "Create a new DataFrame called `clean_data` that is a direct copy of our original `data`, but does not include the colums `['PassengerId', 'Cabin','Name','Ticket']`. Take a look at Panda's [`df.drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) which can be used to delete columns from a given `DataFrame`, especially the parameter `axis` is important here. The method returns a `Dataframe` without the columns that were deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "clean_data = data.drop(['PassengerId', 'Cabin','Name','Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis of the data, we concluded that there were 277 people of which we do not know the age. One could say that the absence of information is also information, but in our case there is no way that our model can calculate a prediction when one of the input variables is absent. When you encounter missing values, you have multiple options for dealing with these missing values: you can delete each row that contains a missing values, you can delete the whole column from the DataFrame, or you can replace the missing values. In our case, deleting the rows would not be an option, as we already do not have a lot of data, and we need as much as possible to create an accurate model. Deleting the column would also not be an option, as the age of a person will probably be a good indicator for whether a person survived or not. The third option, replacing the missing values is good, but we have to find a method that will not skew our results. \n",
    "\n",
    "There are multiple ways of replacing missing values, but it is important to find a method that does not affect the outcome of our model too much. As replacing the values randomly will probably create a lot of outliers and will probably result in our model not being able to learn what correlations there are within the data, we will use a method that replaces missing values with values that are as generic as possible. An easy way to do this is to replace all missing values with a value that is inferred from the data that is available.\n",
    "\n",
    "In the cell below, use Panda's [`df.fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html?highlight=fillna#pandas.DataFrame.fillna) to replace every missing value in the age column of `clean_data` with the average age of all non-missing values.\n",
    "\n",
    "*Hint*: `df.fillna` accepts a value that will be used to replace all missing values. It returns a DataFrame or Series depending on what value you put in. Use `mean()` on the age column to get the average age, and then select the column 'Age' and apply fillna with this mean value. Store the result in the age column to overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>13.002015</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived      Pclass         Age       SibSp       Parch        Fare\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000\n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
       "std      0.486592    0.836071   13.002015    1.102743    0.806057   49.693429\n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    2.000000   22.000000    0.000000    0.000000    7.910400\n",
       "50%      0.000000    3.000000   29.699118    0.000000    0.000000   14.454200\n",
       "75%      1.000000    3.000000   35.000000    1.000000    0.000000   31.000000\n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "clean_data['Age'] = clean_data['Age'].fillna(clean_data['Age'].mean())\n",
    "\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all missing alues in the column \"Age\" have now been replaced with the average. This means that the average of the column has not changed. However, this solution is still not ideal. We have changed the standard deviation and the percentiles, which indicates that we have changed the entire distribution of the data. Sadly there is no way to recover the real data, and for now, we will have to settle with this solution.\n",
    "\n",
    "Finally, we have to transform the columns \"Sex\" and \"Embarked\" from categorical data into a type of data that the model can handle. The column Sex has two categories, male or female, while the port where passengers Embarked has three categories: C = Cherbourg, Q = Queenstown, or S = Southampton. In the previous notebook, we have introduced you to one-hot encoding, which is exactly what we will use here. Panda's has a method (as is to be expected) named [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) that transforms data from categorical to a one-hot encoded set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      "Survived    891 non-null int64\n",
      "Pclass      891 non-null int64\n",
      "Age         891 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Fare        891 non-null float64\n",
      "male        891 non-null uint8\n",
      "C           891 non-null uint8\n",
      "Q           891 non-null uint8\n",
      "S           891 non-null uint8\n",
      "dtypes: float64(2), int64(4), uint8(4)\n",
      "memory usage: 45.4 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare  male  C  Q  S\n",
       "0         0       3  22.0      1      0   7.2500     1  0  0  1\n",
       "1         1       1  38.0      1      0  71.2833     0  1  0  0\n",
       "2         1       3  26.0      0      0   7.9250     0  0  0  1\n",
       "3         1       1  35.0      1      0  53.1000     0  0  0  1\n",
       "4         0       3  35.0      0      0   8.0500     1  0  0  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can drop one of the sexes, as being one of the sexes infers that the passenger can no longer be the other\n",
    "sex = pd.get_dummies(clean_data['Sex'], drop_first=True)\n",
    "\n",
    "# This is not the case for port of embarkment, as there are three possibilities here\n",
    "embark = pd.get_dummies(clean_data['Embarked'])\n",
    "\n",
    "# We no longer need these columns, as we will replace them soon\n",
    "clean_data = clean_data.drop(['Sex','Embarked'],axis=1)\n",
    "\n",
    "# Create the new dataframe with the columns we inferred from the categories in Sex and Embarked\n",
    "clean_data = pd.concat([clean_data, sex, embark],axis=1)\n",
    "\n",
    "clean_data.info()\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final preparations\n",
    "\n",
    "Now that all data is numerical, we can continue to the next step. We have already observed that there is a big difference in scale in data. In his videos, Andrew Ng talks about feature scaling. In feature scaling you make sure that the different features in your dataset take on similar ranges of values. The idea behind this is that after you apply feature scaling, gradient descent will converge more quickly. \n",
    "\n",
    "Scipy's `stat` module has some interesting methods to do this, including the `zscore` method which you might recognise from Andrew's video:\n",
    "\n",
    "$$ Z_j = \\frac{x_j - \\mu_j}{\\sigma_j} $$\n",
    "\n",
    "The basic principle here is that it transforms every value in the column $j$ to a value that indicates how many standard deviations the value originally was away from the mean of the data *in that column*. For example: a $Z$-score of $-2$ would indicate that the value was 2 standard deviations below the average, while a $Z$-score of $0$ would indicate that the value was\n",
    "\n",
    "In the code below, we transform each of the non-categorical datacolumns using the `zscore` method, by applying it to each of the elements within. We do not want to apply this function to our categorical one-hot encoded data, as this would re-introduce ordinality; we applied one-hot encoding to ensure that a something either was part of a class, or wasn't. Note that `zscore` makes sure to use a separate $\\mu$ and $\\sigma$ for every column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>8.910000e+02</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>-8.772133e-17</td>\n",
       "      <td>2.232906e-16</td>\n",
       "      <td>4.386066e-17</td>\n",
       "      <td>5.382900e-17</td>\n",
       "      <td>3.987333e-18</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.566107e+00</td>\n",
       "      <td>-2.253155e+00</td>\n",
       "      <td>-4.745452e-01</td>\n",
       "      <td>-4.736736e-01</td>\n",
       "      <td>-6.484217e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.693648e-01</td>\n",
       "      <td>-5.924806e-01</td>\n",
       "      <td>-4.745452e-01</td>\n",
       "      <td>-4.736736e-01</td>\n",
       "      <td>-4.891482e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.273772e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.745452e-01</td>\n",
       "      <td>-4.736736e-01</td>\n",
       "      <td>-3.573909e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.273772e-01</td>\n",
       "      <td>4.079260e-01</td>\n",
       "      <td>4.327934e-01</td>\n",
       "      <td>-4.736736e-01</td>\n",
       "      <td>-2.424635e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.273772e-01</td>\n",
       "      <td>3.870872e+00</td>\n",
       "      <td>6.784163e+00</td>\n",
       "      <td>6.974147e+00</td>\n",
       "      <td>9.667167e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived        Pclass           Age         SibSp         Parch  \\\n",
       "count  891.000000  8.910000e+02  8.910000e+02  8.910000e+02  8.910000e+02   \n",
       "mean     0.383838 -8.772133e-17  2.232906e-16  4.386066e-17  5.382900e-17   \n",
       "std      0.486592  1.000562e+00  1.000562e+00  1.000562e+00  1.000562e+00   \n",
       "min      0.000000 -1.566107e+00 -2.253155e+00 -4.745452e-01 -4.736736e-01   \n",
       "25%      0.000000 -3.693648e-01 -5.924806e-01 -4.745452e-01 -4.736736e-01   \n",
       "50%      0.000000  8.273772e-01  0.000000e+00 -4.745452e-01 -4.736736e-01   \n",
       "75%      1.000000  8.273772e-01  4.079260e-01  4.327934e-01 -4.736736e-01   \n",
       "max      1.000000  8.273772e-01  3.870872e+00  6.784163e+00  6.974147e+00   \n",
       "\n",
       "               Fare        male           C           Q           S  \n",
       "count  8.910000e+02  891.000000  891.000000  891.000000  891.000000  \n",
       "mean   3.987333e-18    0.647587    0.188552    0.086420    0.722783  \n",
       "std    1.000562e+00    0.477990    0.391372    0.281141    0.447876  \n",
       "min   -6.484217e-01    0.000000    0.000000    0.000000    0.000000  \n",
       "25%   -4.891482e-01    0.000000    0.000000    0.000000    0.000000  \n",
       "50%   -3.573909e-01    1.000000    0.000000    0.000000    1.000000  \n",
       "75%   -2.424635e-02    1.000000    0.000000    0.000000    1.000000  \n",
       "max    9.667167e+00    1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "non_categorical = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "clean_data[non_categorical] = clean_data[non_categorical].apply(zscore)\n",
    "\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that we have adjusted with our `zscore` function now all have a mean that is very close to zero, and a standard deviation that is very close to one. This means that our data is now sufficiently of equal scale and mean-centered.\n",
    "\n",
    "The final thing that we have to do to prepare the data, is separate the input variables from the target variable, and split the dataset into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into target \"y\" and input \"X\"\n",
    "y = clean_data['Survived']\n",
    "X = clean_data.drop('Survived', axis=1)\n",
    "\n",
    "#Split the data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic function\n",
    "\n",
    "For logistic regression, we require a function that generates probabilities; a function that gives outputs between 0 and 1 for **all** inputs. There are many functions that meet this description, but the one that is used in logistic regression is the logistic function:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "Implement the function `logistic_func` that can either take a single value `z`, or an array of values `z`, and compute the *Logistic* function for each.\n",
    "\n",
    "Then, write some code that can plot the results for the logistic function between $-5$ and $5$. Use Numpy's [`np.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html?highlight=linspace#numpy.linspace) to generate a range of $z$ values and use your `logistic_func` to calculate the corresponding $g$ values. Can you see why this function is also sometimes called the *Sigmoid* function? (The term \"sigmoid\" means S-shaped.)\n",
    "\n",
    "*Hint:* Numpy built-in functions and basic arithmetic operations work on both Numpy arrays and single values. When the function detects that its input is a Numpy array, the operation it applies will be applied to each element in the array seperately, the result being a Numpy array of the same shape as the input. This is called *element-wise application*. Write your `logistic_func` in such a way that it can compute $g$ values for scalars or for entire vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfGElEQVR4nO3deXxU5d338c+P7HuAJGwJOwJhFQLuSl1aFIVqq0Klrjc83r1t7a22dSta26ett61WK8qN1rUWikuVKkrdQJ9aEZA1rAkgISxJCNnJfj1/JNqIIANMcmb5vl8vXjNz5pD5jobv65przjmXOecQEZHg18nrACIi4h8qdBGREKFCFxEJESp0EZEQoUIXEQkRkV69cFpamuvbt69XLy8iEpRWrlxZ4pxLP9xznhV63759WbFihVcvLyISlMzssyM9pykXEZEQoUIXEQkRKnQRkRChQhcRCRFHLXQze8rMisxs/RGeNzN7xMzyzGytmY3xf0wRETkaX0bozwATv+b5C4FBrX9mAo+feCwRETlWRy1059wHQOnX7DIFeM61+BhINbMe/gooIiK+8cdx6L2AgjaPd7Vu23PojmY2k5ZRPL179/bDS4uIBAbnHHWNzVTUNlBZ20hlbSM1dY1U1TVSU9/UettIVV0T5w3JYFRWqt8zdOiJRc65ucBcgJycHF2IXUQCUlOzo6ymnpKqekqq6iipqqO0up6ymgbKauopO9jAgZoGylvvtxR4Aw1NvtVaRlJMwBZ6IZDV5nFm6zYRkYBzsL6JXQdq2FNey97yWvZW1LKnvJZ9rbfFlXWUVtfRfIRuTo6NpHNCNKlxUaTGR9OnawLJcZEkxUaRFNtymxwbSVJsJAnRkSTEtP6JjiA+JpL4qAg6dbJ2eW/+KPSFwE1mNh84BSh3zn1lukVEpKOU1zSQV1xJfnE1BaU1FJTWsLO0hoIDBymurPvK/mmJ0XRLjqVnSiyjs1JIS4yha0I0aUkxdE2IIT0pmi4JMaTERRHRTmXsD0ctdDObB0wA0sxsF3APEAXgnJsDLAIuAvKAGuC69gorItJW+cEGcneXs3lvJXlFVeQVVZFfXEVJVf0X+3Qy6JkaR1bneM4dnEFWlziyusTTMzWO7smxZCTHEBMZ4eG78J+jFrpzbtpRnnfAf/ktkYjIYZRU1bFuVzm5u8tZX1hB7p5yCkoPfvF8SlwUAzMSOXdIBgMzEhmYkUj/tER6dY4jKiI8zqH07GqLIiJH4pxje0k1K3YcYPmOUlZ8doDtJdVfPN+3azwje6UybXxvhvVMYWiPJNITYzAL3OmQjqBCF5GAUFh2kKWbi/lgSzHLd5Syv7pl2iQ1PoqcPl24clwWo7NSye6ZTHJslMdpA5MKXUQ8UdvQxCfbS1m6pZilW4rJK6oCoGdKLBMGZzCub2dy+namf1piux0VEmpU6CLSYWobmnh/UxGvr93De5uKONjQRHRkJ07p14Wp47KYMDidAemJYT91crxU6CLSrmobmvhgSzGvr93DOxv3UVPfRNeEaC4b04vzh3bjlP5diI9WFfmD/iuKSLvI3V3OC8t28vfVu6msa6RzfBRTRvfk4pE9OaVfFyLD5MiTjqRCFxG/OVjfxOtrd/PCsp2sLigjJrITk0b2YMroXpw+oGvYHD7oFRW6iJyw7SXVPPvRDl75dBcVtY0MzEjknkuyuezkTFLidURKR1Ghi8hx27ingtnv57Fo3R4iO3Vi4vDuXHVKb8b366IvNj2gQheRY7a6oIxH38vjnY37SIyJZObZA7jhzH6kJ8V4HS2sqdBFxGcrdpTy8Ltb+XBrCSlxUfz3+Sdx7el9Na0SIFToInJUBaU1/ObNjSxat5e0xGjuuHAIV53ah8QYVUgg0f8NETmiytoGHluSz58+3E5EJ+OWC05ixln9iYsOjasThhoVuoh8RVOz46WVBTyweAslVXVcNqYXP/3WELqnxHodTb6GCl1EviR3dzk/fWktubsrGNunM09ek8PodlguTfxPhS4iADQ0NTP7/TwefS+PzgnRPDLtZC4Z2UOHHwYRFbqIsHFPBbe9uIbc3RV8e3RP7p08jNT4aK9jyTFSoYuEsYamZuYsyeeR97aSEhfFnOljmTi8u9ex5Dip0EXC1PaSan40bxXrCsu5ZFRPfjF5GF0SNCoPZip0kTD0j9y93LpgDRERxmNXjeGiET28jiR+oEIXCSNNzY7f/2Mzjy3JZ2RmCo9dNYbMzvFexxI/UaGLhInS6np+NG8V/y+vhGnjs7jnkmHERukEoVCiQhcJA6sLyvjBn1dSUl3P/d8ZwZXjensdSdqBCl0kxL28chd3vLKOjOQYXr7xdEZkpngdSdqJCl0kRDnnmLN0G/e/tYnTB3Rl9vfG0FlHsYQ0FbpICGpudvzyjQ08/c8dTB7Vk99dPoroSC3/FupU6CIhpq6xiVsWrOGNtXu4/ox+3D1pKJ066fT9cKBCFwkhlbUN/J/nV/JR/n7uvGgIM87qr2uxhBEVukiIKKqs5dqnlrNlXyUPXjGKy8Zkeh1JOpgKXSQEFFXUMnXux+ytqOXJa3KYMDjD60jiARW6SJArqarje08uY29FLc9dP56cvl28jiQe8elrbzObaGabzSzPzG4/zPO9zex9M1tlZmvN7CL/RxWRQx2ormf6k8vYdaCGp64dpzIPc0ctdDOLAGYDFwLZwDQzyz5kt7uBBc65k4GpwGP+DioiX1Ze08D0Py1jW0k1T149jlP7d/U6knjMlxH6eCDPObfNOVcPzAemHLKPA5Jb76cAu/0XUUQOVVnbwNVPf8LWfVXM/f5YzhyU5nUkCQC+FHovoKDN412t29q6F5huZruARcAPD/eDzGymma0wsxXFxcXHEVdEqusaufbp5eQWlvPYVWP0Bah8wV+njk0DnnHOZQIXAc+b2Vd+tnNurnMuxzmXk56e7qeXFgkfdY1N/MezK1hdUMYfp53M+dndvI4kAcSXQi8Esto8zmzd1tYNwAIA59y/gFhAnwFF/Mg5x89eWsu/tu3n95eP4kItSiGH8KXQlwODzKyfmUXT8qXnwkP22QmcB2BmQ2kpdM2piPjRQ29v4dXVu/nJtwbz7ZMPnfUU8aHQnXONwE3AYmAjLUez5JrZfWY2uXW3W4EZZrYGmAdc65xz7RVaJNwsWFHAI+/lcWVOFj+YMMDrOBKgfDqxyDm3iJYvO9tum9Xm/gbgDP9GExGAf+aVcOcr6zhrUBq/unS4rs0iR6TraYoEsC37Krnx+ZUMSE9k9lVjiIrQP1k5Mv12iASoosparnt6ObHRETx13TiSY6O8jiQBToUuEoAO1rccnlhaXc9T14yjV2qc15EkCOjiXCIBxjnHXa+uY11hOXO/n6M1QMVnGqGLBJgXlu3klU8Lufm8QVygE4fkGKjQRQLIqp0H+MXfc5kwOJ0fnTvI6zgSZFToIgGipKqOH7zwKd2SY/nDlaO1DqgcM82hiwSAxqZmfjRvFaXV9bz8n6eTGh/tdSQJQip0kQDw+7e38FH+fh747kiG99KXoHJ8NOUi4rG31u/l8SX5TBvfm8tzso7+F0SOQIUu4qHtJdXc9uIaRmWmcO/kQxcCEzk2KnQRj9Q3NnPz/FVEdDIemz6WmMgIryNJkNMcuohHHnpnC2t3lTNn+hidCSp+oRG6iAc+yi9hztJ8po7LYuJwLVQh/qFCF+lgZTX13PLXNfTrmsCsSzRvLv6jQhfpQM457nhlHfur63h46snER2vWU/xHhS7SgRasKODN9Xu59ZuDddEt8TsVukgH2VZcxb0LN3D6gK7MPKu/13EkBKnQRTpAyyGKq4mJ6sSDV+g6LdI+NIEn0gEefncL6wrLmTN9LN1TYr2OIyFKI3SRdramoIzHl+Rz+dhMJg7v7nUcCWEqdJF2VNfYxG0vriEjKZa7L9YhitK+NOUi0o4efmcrW4uqePq6caTEaZFnaV8aoYu0kzUFZcxZms8VOZl8Y3CG13EkDKjQRdpB26mWuyZpqkU6hqZcRNqBplrECxqhi/iZplrEKyp0ET/SVIt4SVMuIn70yLuaahHvaIQu4icbdlcwZ+k2vjtWUy3iDRW6iB80NTvueGUtqXFR3D1pqNdxJEz5VOhmNtHMNptZnpndfoR9rjCzDWaWa2Z/8W9MkcD23L92sGZXObMuySY1PtrrOBKmjjqHbmYRwGzgAmAXsNzMFjrnNrTZZxBwB3CGc+6AmenzpoSNwrKDPLB4M+eclM7kUT29jiNhzJcR+nggzzm3zTlXD8wHphyyzwxgtnPuAIBzrsi/MUUCk3OOWa+uxzn41beHY6bL4op3fCn0XkBBm8e7Wre1dRJwkpn908w+NrOJh/tBZjbTzFaY2Yri4uLjSywSQBat28u7m4q49ZsnkdUl3us4Eub89aVoJDAImABMA54ws9RDd3LOzXXO5TjnctLT0/300iLeKK9p4J6FuQzvlcy1p/f1Oo6IT4VeCGS1eZzZuq2tXcBC51yDc247sIWWghcJWb99axMHaur57WUjiYzQAWPiPV9+C5cDg8ysn5lFA1OBhYfs8yoto3PMLI2WKZhtfswpElA+2V7KvE92csOZ/RjeS4s9S2A4aqE75xqBm4DFwEZggXMu18zuM7PJrbstBvab2QbgfeAnzrn97RVaxEt1jU3c8cpaMjvH8ePz9UFUAodPp/475xYBiw7ZNqvNfQfc0vpHJKQ98cE28ourefq6ccRH6+oZEjg08SdyDHbur+GP7+Vx0YjuOr1fAo4KXcRHzjlmLVxPZCdj1sXDvI4j8hUqdBEfvbV+L0s2F3PLNwfTPSXW6zgiX6FCF/FBVV0jv/j7BrJ7JHPNaX28jiNyWPpGR8QHD729hX2VtTw+fYyOOZeApd9MkaPYsLuCZz7awbTxvTm5d2ev44gckQpd5Gs0NzvuenUdqXFR/OxbQ7yOI/K1VOgiX2P+8gJW7SzjrklDSYnXknIS2FToIkewv6qO+9/axKn9u3DpyYdeYFQk8KjQRY7gN29uoqa+Udc5l6ChQhc5jOU7Snlp5S7+46z+DMxI8jqOiE9U6CKHaGhq5u6/radXahw/PHeg13FEfKZCFznEsx/tYPO+Su65JFsX35KgokIXaWNveS0Pvb2Fc4dkcEF2N6/jiBwTFbpIG798YwONzY57LxmmL0Il6KjQRVp9uLWYN9bu4aZvDKR3Vy34LMFHhS5CyypEs17LpV9aAjPP6e91HJHjom98RIC5S7exvaSa564fT0xkhNdxRI6LRugS9gpKa3j0/TwmjezB2Selex1H5Lip0CWsOeeY9VrLKkQ/n5TtdRyRE6JCl7C2OHcv728u5r8vOEmrEEnQU6FL2Pp8FaKhPZK59vS+XscROWEqdAlbf3h7C3sravm/lw7XKkQSEvRbLGFp454Knv5oB1PH9WaMViGSEKFCl7DT3Oy462+tqxBNHOx1HBG/UaFL2PnrigI+3VnGnRcNJTU+2us4In6jQpewsr+qjt++uYlT+nXhsjFahUhCiwpdwsqvF22iuk6rEEloUqFL2Fi2bT8vf7qLmWf3Z1A3rUIkoUeFLmGhrrGJO/+2jszOcfzw3EFexxFpF7o4l4SFx5fkk19czTPXjSMuWhffktDk0wjdzCaa2WYzyzOz279mv++YmTOzHP9FFDkxeUWVPPZ+PlNG92TC4Ayv44i0m6MWuplFALOBC4FsYJqZfeUqRmaWBNwMLPN3SJHj1dzsuOOVdcRFR/Dzi3XxLQltvozQxwN5zrltzrl6YD4w5TD7/RK4H6j1Yz6REzJ/eQHLdxzgrklDSUuM8TqOSLvypdB7AQVtHu9q3fYFMxsDZDnn3vi6H2RmM81shZmtKC4uPuawIseiqKKW37y5kdP6d+XysZlexxFpdyd8lIuZdQIeBG492r7OubnOuRznXE56uhYSkPZ1799zqWts5teXjdAx5xIWfCn0QiCrzePM1m2fSwKGA0vMbAdwKrBQX4yKl97esI9F6/Zy83mD6JeW4HUckQ7hS6EvBwaZWT8ziwamAgs/f9I5V+6cS3PO9XXO9QU+BiY751a0S2KRo6iqa2TWa+sZ3C2JGWdpwWcJH0ctdOdcI3ATsBjYCCxwzuWa2X1mNrm9A4ocq98t3szeilp+850RREfq3DkJHz6dWOScWwQsOmTbrCPsO+HEY4kcn0+2l/Lsv3Zw9al9dJ1zCTsavkjIOFjfxE9eWkNm5zh+OnGI13FEOpxO/ZeQ8T+LN/HZ/hrmzTiVhBj9akv40QhdQsIn20t55qMdXHNaH04b0NXrOCKeUKFL0NNUi0gLfS6VoKepFpEWGqFLUNNUi8i/qdAlaGmqReTL9PlUgpamWkS+TCN0CUr/zCvh6X9qqkWkLRW6BJ0D1fXcsmA1AzMSuf3CoV7HEQkYKnQJKs45fvbyWkqr63l46mitDyrShgpdgspfPtnJPzbs46ffGsKwnilexxEJKCp0CRp5RZX88vUNnDUojRvO7Od1HJGAo0KXoFDX2MSP5q0mLiqC310+ik6dtAKRyKF0rJcEhd8t3syGPRU8cXUO3ZJjvY4jEpA0QpeA9+HWYp74cDtXndKbC7K7eR1HJGCp0CWg7a+q49YFaxiYkcjdk7K9jiMS0FToErAam5r54bxVlB1s0CGKIj5QoUvA+v3bW/gofz+/+vZwHaIo4gMVugSkxbl7eXxJPtPG9+aKnCyv44gEBRW6BJxtxVXctmANIzNTuOcSzZuL+EqFLgGlpr6RG/+8ksgI47GrxhAbpXlzEV/pOHQJGM45bn95HVuLqnju+vFkdo73OpJIUNEIXQLGsx/tYOGa3dz2zcGcNSjd6zgiQUeFLgHhk+2l/OqNjZw/tBv/ec4Ar+OIBCUVunhue0k1M59fQe8u8fz+Cl2nReR4qdDFU6XV9Vz39Cd0MuPp68aREhfldSSRoKUvRcUztQ1NzHhuBbvLa5k341T6dE3wOpJIUNMIXTzR3Oy47cU1rPzsAA9dMZqxfTp7HUkk6KnQxRMP/GMzr6/dwx0XDmHSyB5exxEJCSp06XDzPtnJ40vy+d4pvZl5dn+v44iEDJ8K3cwmmtlmM8szs9sP8/wtZrbBzNaa2btm1sf/USUULNlcxN2vrueck9K5b/IwzHREi4i/HLXQzSwCmA1cCGQD08zs0AtsrAJynHMjgZeA//F3UAl+H2/bz41/XsngbknMvmoMkRH6gCjiT778ixoP5Dnntjnn6oH5wJS2Ozjn3nfO1bQ+/BjI9G9MCXYrdpRy/TPLyeocz/M3jCcxRgdYifibL4XeCyho83hX67YjuQF483BPmNlMM1thZiuKi4t9TylBbXVBGdc+vZzuybG8MOMUuibGeB1JJCT59TOvmU0HcoAHDve8c26ucy7HOZeTnq5rdYSD9YXlfP9Py+iSEM1fZpxKRpIWeBZpL7587i0E2q4wkNm67UvM7HzgLuAc51ydf+JJMNu4p4Lpf1pGcmwUf5lxCt1TVOYi7cmXEfpyYJCZ9TOzaGAqsLDtDmZ2MvC/wGTnXJH/Y0qw2bqvkqueXEZsZATzZpyqS+GKdICjFrpzrhG4CVgMbAQWOOdyzew+M5vcutsDQCLwopmtNrOFR/hxEgbW7Spn2hMfE9HJ+MuMU+jdVWUu0hF8OtTAObcIWHTItllt7p/v51wSpD7YUsyNf15J5/honrthPP3TE72OJBI2dOyY+M2rqwq57cU1DMxI5Nnrx9MtWXPmIh1JhS5+8eSH2/jVGxs5tX8X5l6dQ3KsLoMr0tFU6HJCmpsdv3lzI098uJ2LRnTnwStGa2FnEY+o0OW41TY08bOX1/La6t1cc1ofZl0yjAitNiTiGRW6HJeC0hp+8MKnrCss5yffGswPJgzQhbZEPKZCl2O2dEsxN89fRVOz48mrczg/u5vXkUQEFbocg+Zmx+z383jwnS0M7pbEnOlj6ZumZeNEAoUKXXxSfrCBW/66mnc3FXHpyb349aUjiIvWl58igUSFLke1aucBfvzX1ewuO8h9U4bx/VP7aL5cJACp0OWIahua+MM7W5n7QT7dk2OZP/M0LeYsEsBU6HJYawrKuO3FNWwtqmLquCzumjSUJJ0sJBLQVOjyJXWNTTzy7lbmLN1GemIMz14/nnNO0rXrRYKBCl2+sPKzA9z5yjo276vk8rGZ3H1xNilxGpWLBAsVurC77CD3v7WJ11bvpltyDE9fO45vDMnwOpaIHCMVehirqW9kztJtzP0gH+fgh+cO5MZzBpCgBZxFgpL+5Yah5mbHq6sLuf+tTeyrqOOSUT352cTBWlVIJMip0MNIc7Pjrdy9PPpeHhv2VDAqM4XZ3xtDTt8uXkcTET9QoYeBxqZmFq7ZzWNL8skrqqJfWgIPXTmKKaN60UlXRxQJGSr0EFbX2MTLKwuZszSfnaU1DOmexB+nncxFI3roMrciIUiFHoIKyw4y/5OdzF9eQHFlHaOyUvn5xdmcNyRDI3KREKZCDxFNzY4lm4t4YdlOlmwuwgHfGJzBdWf05cyBabr2ikgYUKEHuR0l1Sxcs5v5n+xkd3kt6Ukx/Nc3BnLluCwdtSISZlToQaigtIY31u3h9bW7WV9YAcCZA9P4+cXZnJ/djaiITh4nFBEvqNCDgHOOHftreHfjPv6+dg9rCsoAGJWVyt2ThnLhiB70So3zOKWIeE2FHqCq6xr5V/5+lm4pZumWYnaW1gAwolcKt184hEkjepDVRVMqIvJvKvQAcbC+idUFZaz8rJSP8vezfEcpDU2O+OgITh+Qxoyz+zPhpHSVuIgckQrdI0WVtXz6WUuBL99xgPWF5TQ2OwCGdE/i+jP6cc5J6Yzt25mYSC31JiJHp0JvZ845dh04yPrCcnJ3V7B+d8ttcWUdANGRnRidmcqMs/szrm9nxvTuTGp8tMepRSQYqdD9pLGpmZ2lNeQXV5NXVEVeURX5xVXkF1VRWdcIQEQnY1BGImcNSmN4zxRGZqYwIjNFI3AR8QsVuo+amx2lNfXsKaul4EANO0trKCj9921h2UEamtwX+2ckxTAwI5FLx/RiSPdkhvVMZnD3JGKjVN4i0j7CvtAbm5opra6npKqekqo69lfXUVJZT1FlLXsr6thbfpA95bUUVdRR39T8pb/bOT6K3l3iGdYrhYnDezAgPYGBGYkMyEgkWetvikgH86nQzWwi8DAQATzpnPvtIc/HAM8BY4H9wJXOuR3+jXp4zjlqG5qpqmukuq6RytpGKmsbqGi9bXncSNnBespqGiirqedATQPlBxs4UNOy7XBiIjvRIyWWbsmx5PTpTPeUOLonx9A9JY7eXeLJ6hKnRZNFJKActdDNLAKYDVwA7AKWm9lC59yGNrvdABxwzg00s6nA/cCV7RH4r8t38r9Lt1Fd30h1XRPV9Y04d/S/lxQTSUp8FJ3jo0mNjyKrSzypcVF0SYgmLTGatMQY0pJi6JoQTVpSDEkxkbr+iYgEFV9G6OOBPOfcNgAzmw9MAdoW+hTg3tb7LwGPmpk550vVHpsuCTFk90wmITqShJhIEmIiWm6jI4iPjiQpNpLkuKiW29iW28SYSCJ1OryIhDhfCr0XUNDm8S7glCPt45xrNLNyoCtQ0nYnM5sJzATo3bv3cQW+ILsbF2R3O66/KyISyjp02Oqcm+ucy3HO5aSnp3fkS4uIhDxfCr0QyGrzOLN122H3MbNIIIWWL0dFRKSD+FLoy4FBZtbPzKKBqcDCQ/ZZCFzTev+7wHvtMX8uIiJHdtQ59NY58ZuAxbQctviUcy7XzO4DVjjnFgJ/Ap43szyglJbSFxGRDuTTcejOuUXAokO2zWpzvxa43L/RRETkWOhYPhGREKFCFxEJESp0EZEQYV4djGJmxcBnnrz4iUnjkBOmwkC4vedwe7+g9xxM+jjnDnsij2eFHqzMbIVzLsfrHB0p3N5zuL1f0HsOFZpyEREJESp0EZEQoUI/dnO9DuCBcHvP4fZ+Qe85JGgOXUQkRGiELiISIlToIiIhQoV+AszsVjNzZpbmdZb2ZGYPmNkmM1trZn8zs1SvM7UXM5toZpvNLM/Mbvc6T3szsywze9/MNphZrpnd7HWmjmJmEWa2ysxe9zqLv6jQj5OZZQHfBHZ6naUDvA0Md86NBLYAd3icp120WT/3QiAbmGZm2d6maneNwK3OuWzgVOC/wuA9f+5mYKPXIfxJhX78HgJ+CoT8t8rOuX845xpbH35MyyInoeiL9XOdc/XA5+vnhizn3B7n3Ket9ytpKbhe3qZqf2aWCUwCnvQ6iz+p0I+DmU0BCp1za7zO4oHrgTe9DtFODrd+bsiX2+fMrC9wMrDM2yQd4g+0DMiavQ7iTz5dDz0cmdk7QPfDPHUXcCct0y0h4+ver3PutdZ97qLlI/oLHZlN2p+ZJQIvAz92zlV4nac9mdnFQJFzbqWZTfA6jz+p0I/AOXf+4bab2QigH7DGzKBl+uFTMxvvnNvbgRH96kjv93Nmdi1wMXBeCC8v6Mv6uSHHzKJoKfMXnHOveJ2nA5wBTDazi4BYINnM/uycm+5xrhOmE4tOkJntAHKcc8F41TafmNlE4EHgHOdcsdd52kvrAudbgPNoKfLlwPecc7meBmtH1jIqeRYodc792Os8Ha11hH6bc+5ir7P4g+bQxRePAknA22a22szmeB2oPbR+8fv5+rkbgQWhXOatzgC+D5zb+v92devIVYKQRugiIiFCI3QRkRChQhcRCREqdBGREKFCFxEJESp0EZEQoUIXEQkRKnQRkRDx/wEB0fss9QJIPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logistic_func(z):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "assert logistic_func(0) == 0.5\n",
    "assert logistic_func(1) == np.e / (1 + np.e)\n",
    "assert np.allclose(logistic_func(np.array([0,1])), np.array([0.5, np.e/ (1 + np.e)]))\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "z = np.linspace(-5, 5)\n",
    "g = logistic_func(z)\n",
    "\n",
    "plt.plot(z, g)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis function\n",
    "\n",
    "We now have a method of transforming our inputs into probabilities, and can now start working on the hypothesis function which will generate an output given a set of inputs $X$ -- in our case the features we created from the Titanic data set -- and a given set of weights $\\theta$ -- which we will try to learn from our dataset.\n",
    "\n",
    "For logistic regression, we can use the logistic function $g$ to transform the hypothesis we used in multivariate linear regression to a hypothesis function that only generates probabilities (results from 0 to 1):\n",
    "\n",
    "$$ h_\\theta(x^1) = g(\\theta^Tx^1) = g(z^1) = \\frac{1}{1+e^{-z^1}} = \\frac{1}{1+e^{-\\theta^Tx^1}}$$\n",
    "$$ h_\\theta(x^2) = g(\\theta^Tx^2) = g(z^2) = \\frac{1}{1+e^{-z^2}} = \\frac{1}{1+e^{-\\theta^Tx^2}}$$\n",
    "$$ h_\\theta(x^3) = g(\\theta^Tx^3) = g(z^3) = \\frac{1}{1+e^{-z^3}} = \\frac{1}{1+e^{-\\theta^Tx^3}}$$\n",
    "$$\\dots$$\n",
    "$$ h_\\theta(x^n) = g(\\theta^Tx^n) = g(z^n) = \\frac{1}{1+e^{-z^n}} = \\frac{1}{1+e^{-\\theta^Tx^n}}$$\n",
    "\n",
    "\n",
    "Herein, $z^i$ is the linear combination of the i-th input vector $x^i$ and the parameter vector $\\theta$.\n",
    "\n",
    "Now, in our text on [the logistic function](#The-logistic-function) just above this text, we discussed how Numpy is able to apply functions element-wise. Using the matrix multiplication we have also used in multivariate linear regression, we can generate a column vector of predictions that have not yet been transformed into probabilities: \n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "x_0^1 & x_1^1 & x_2^1 & \\cdots & x_n^1 \\\\ \n",
    "x_0^2 & x_1^2 & x_2^2 & \\cdots & x_n^2 \\\\ \n",
    "x_0^3 & x_1^3 & x_2^3 & \\cdots & x_n^3 \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_0^m & x_1^m & x_2^m & \\cdots & x_n^m \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\\n",
    "\\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{array} \\right]\n",
    "= \\left[\\begin{array}{cccc}\n",
    "z^1\\\\\n",
    "z^2\\\\\n",
    "z^3\\\\\n",
    "\\dots \\\\\n",
    "z^m \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "We can then use our logistic function to apply this function to each of the elements in this column vector of predictions:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cccc}\n",
    "g(z^1)\\\\\n",
    "g(z^2)\\\\\n",
    "g(z^3)\\\\\n",
    "\\dots \\\\\n",
    "g(z^m) \\\\\n",
    "\\end{array} \\right]\n",
    "=\n",
    "\\left[\\begin{array}{c} h_{\\theta}(x^1) \\\\ h_{\\theta}(x^2) \\\\\n",
    "h_{\\theta}(x^3) \\\\ \\vdots \\\\ h_{\\theta}(x^m) \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "Due to Numpy applying the function to each element, instead of the whole vector at once, the only thing that really changes in our programming between the multivariate linear regression model and our new logistic regression model is that we use our logistic function to transform the whole vector of values to be between 0 and 1. To get these prediction values, we can thus just use our old function `linear_model`.\n",
    "\n",
    "First, copy your implementation of `linear_model`, which you have already implemented in the polynomial regression notebook.\n",
    "\n",
    "Then, write the function `add_x0`, which takes a matrix of `X` values, and returns that same matrix with a column of ones added to the front. For this function you should use [np.ones](https://numpy.org/doc/stable/reference/generated/numpy.ones.html) to create the column of ones. Then, take a look at [`np.concatenate`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html?highlight=concat#numpy.concatenate), which enables you to combine the column vector of ones to your matrix `X`.\n",
    "\n",
    "Then write the function `logistic_model`, which takes a matrix of `X` values, a weight vector `theta`, and calculates $h(X)$ using the functions `add_x0`, `logistic_func`, and `linear_model`. Make sure to first add the bias, or the shapes of the matrix `X` and vector `theta` won't match up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X, theta):\n",
    "    ### YOUR (COPIED) SOLUTION HERE\n",
    "    return X.dot(theta)\n",
    "\n",
    "def add_x0(X):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return np.concatenate((np.ones((X.shape[0],1)), X), axis=1)\n",
    "\n",
    "def logistic_model(X, theta):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    X = add_x0(X)\n",
    "    \n",
    "    return logistic_func(linear_model(X, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Now that we can make predictions using an input matrix $X$, and our parameter weights $\\theta$, we can evaluate the cost of our model. To be able to apply gradient descent, we will need a cost function that is convex when we use our logistic model to make predictions. This cost function is explained in great detail by Andrew Ng, so we will not explain it again here. The logistic regression cost function is given as:\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^M y^i log(h_\\theta(X^i)) + (1 - y^i) log(1 - h_\\theta(X^i))$$\n",
    "\n",
    "Implement the function `logistic_cost`. Use your `logistic_model` function to calculate the hypothesis vector $h_\\theta(X)$. Just as with the cost function for Polynomial Regression, it is actually possible to write this whole function using linear algebra. \n",
    "\n",
    "*Hint:* By using `np.log` to apply the logarithm function to every element of a vector, you can calculate the logarithm of every element in the vector $h_\\theta(X)$ at once. If you are stuck on how to do this, first try to implement the function by looping over $i$, calculating the cost for each of the separate input and output vectors and summing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost(theta, X, y):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    h_X = logistic_model(X, theta)\n",
    "\n",
    "    return -(y.dot(np.log(h_X)) + (1 - y).dot(np.log(1 - h_X))) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Gradient terms for Logistic Regression\n",
    "\n",
    "Manually taking the whole partial derivative of this cost function can be a little time consuming, but in order to still practice this skill, we'll just do a part of this derivative instead. For this part of the notebook, you should consult the `logistic_derivative.pdf` supplement, included as a part of the assignment. Read sections 1, 2 and 3 in the supplement, and then complete the steps of the partial derivative below, labeling each of the steps with their respective rules (as with the linear regression assignment).\n",
    "\n",
    "$$\\frac{\\partial E_{\\mathbf{\\theta}}^i}{\\partial h_{\\mathbf{\\theta}}^i} =\n",
    "\\frac{\\partial}{\\partial h_{\\mathbf{\\theta}}^i} -y^i log(h_{\\mathbf{\\theta}}^i) -\n",
    "(1 - y^i) log(1 - h_{\\mathbf{\\theta}}^i)$$\n",
    "\n",
    "**TODO:** *Your steps, each labeled with what rules you've applied, should go here*.\n",
    "\n",
    "$$\\frac{\\partial E_{\\mathbf{\\theta}}^i}{\\partial h_{\\mathbf{\\theta}}^i} =\n",
    "\\frac{-y^i}{h_{\\mathbf{\\theta}}^i} +\n",
    "\\frac{1 - y^i}{1 - h_{\\mathbf{\\theta}}^i}$$\n",
    "\n",
    "Section 4 of the supplement is optional, but definitely read sections 5 and 6 before moving on to the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Vector and Gradient Descent\n",
    "\n",
    "In the end, the partial derivatives for each of the $\\theta$ parameters look the exactly same as for polynomial regression. While the applied hypothesis function $h$ changes, the rest of the derivative does not, even though the cost function is completely different. As a result, the equations for the $n+1$ partial derivatives for each of the $n+1$ parameters, in the notation of the theory videos, then just become:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_0$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_1$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_2} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_2$$\n",
    "$$\\dots$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_n} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_n$$\n",
    "\n",
    "Copy your implementation of `gradient_vector` and `gradient_descent`, which you have already implemented in the polynomial regression notebook. *Change `gradient_vector` to make use of the new hypothesis function `logistic_model`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6604159914274931\n",
      "[-3.01634284e-02 -5.03008451e-02 -1.30059812e-02 -2.51839250e-05\n",
      "  1.45816825e-02  3.74960032e-02 -5.65759930e-02  2.67541741e-03\n",
      " -1.90510352e-03 -3.09337423e-02]\n"
     ]
    }
   ],
   "source": [
    "def gradient_vector(theta, X, y):\n",
    "    ### YOUR (COPIED) SOLUTION HERE\n",
    "    h_X = logistic_model(X, theta)\n",
    "    \n",
    "    X = add_x0(X)\n",
    "    \n",
    "    return (h_X - y).dot(X) / X.shape[0]\n",
    "    \n",
    "def gradient_descent(X, y, theta, alpha, thres=10**-6):\n",
    "    ### YOUR (COPIED) SOLUTION HERE\n",
    "    cost = logistic_cost(theta, X, y)\n",
    "    cost_hat = cost + 1\n",
    "    \n",
    "    while (abs(cost - cost_hat) > thres):\n",
    "        cost = cost_hat\n",
    "        theta = theta - alpha * gradient_vector(theta, X, y)\n",
    "\n",
    "        cost_hat = logistic_cost(theta, X, y)\n",
    "\n",
    "    print(cost)\n",
    "    return theta\n",
    "\n",
    "# Find the theta vector that minimizes the cost function\n",
    "theta = gradient_descent(X_train, y_train, np.zeros(X_train.shape[1] + 1), 10**-5)\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We now have a learned vector of weights $\\theta$ that we can use to make predictions for our test samples `X_test`. We can re-use our hyptohesis function to calculate the probability that each of the samples in `X_test` is a survivor or not: $p(y=1|X_{test},\\theta)$. These estimates of the probability that $y=1$ still need to be transformed into boolean predictions; we predict that either the person has survived $y=1$, or that the person has not survived $y=0$.\n",
    "\n",
    "Write the function `predict`, that uses a matrix of input values `X` and a vector of weights `theta`, and transforms them into a vector of boolean predictions. Use `logistic_model` to generate predictions, and then use a decision boundary of $h_\\theta(x) \\leq 0.5$ to determine when $y=1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return logistic_model(X, theta) >= 0.5\n",
    "\n",
    "predictions = predict(X_test, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining accuracy\n",
    "\n",
    "Of course we would like to see how *accurate* our model is now that we have trained it and generated predictions! Implement the function `calc_accuracy` that accepts a vector of `predictions` and a vector of truth values `y`. The function should return the factor of correct predictions (predictions where the value in `predictions` is equal to the value in `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6865671641791045\n"
     ]
    }
   ],
   "source": [
    "def calc_accuracy(predictions, y):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return sum(predictions == y) / len(y)\n",
    "\n",
    "print(calc_accuracy(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The accuracy we have just determined is of course one of the primary goals of building a model; we generally want a Machine Learning model to be as accurate as possible. However, it does not give much insight into in what way our model was accurate. As an example, let's say that we want to diagnose whether a patient has a virus. Of course we want our model to diagnose as accurately as possible, but we would also prefer that a patient that has the virus was not diagnosed as negative (not having the virus), while the opposite is not as important. Falsely diagnosing a patient as negative could become very harmful for the patient and their environment. \n",
    "\n",
    "To give better insight into the performance of a classification algorithm, typically, a confusion matrix is made. The confusion matrix is a table layout that allows quick identification of the performance of a classification algorithm. Each *row* in the matrix represents the data that got predicted as a specific class, while each *column* represents the data that is actually in a class:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> TP </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> FP </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\">FN</td>\n",
    "            <td style=\"border: 1px solid black;\"><b> TN </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Where:\n",
    "- P = Positive\n",
    "- N = Negative\n",
    "- TP = True Positive, denoting every value that was correctly predicted as positive\n",
    "- FP = False Positive, denoting every value that was predicted as positive but was actually negative\n",
    "- TN = True Negative, denoting every value that was correctly predicted as negative\n",
    "- FN = False Negative, denoting every value that was predicted as negative but was actually positive\n",
    "\n",
    "For example, using the example of diagnosis of patients, let's say that 100 people take a test, and of these people, 80 actually have the virus. We have two different testing kits that are used to diagnose the patients, and these are the resulting confusion matrices:\n",
    "\n",
    "\n",
    "##### Testing kit 1\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 56 </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 1 </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 24 </td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 19 </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### Testing kit 2\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 62 </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 10 </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 18 </td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 10 </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Now, we can learn a lot from these confusion matrices. First, the calculated accuracy of our first virus diagnosis testing kit is 77%, as 56 people are correctly diagnosed as positive, and 19 people are correctly diagnosed as negative.  The calculated accuracy for the second testing kit is 72%. However, using the first testing kit 24 people have been incorrectly diagnosed as negative, while for the second this was 18. Now, arguably, we would want to use testing kit 2, as even though the total accuracy of this testing kit is lower, this kit produces fewer (harmfull) false negatives. \n",
    "\n",
    "Implement the function `confusion_matrix` that, given `predictions` and truth values `y`, creates a 2-dimensional Numpy array that contains the number of TP, FP, FN, and TN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[260  84]\n",
      " [233 157]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(predictions, y):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    TP = sum(predictions == y & predictions)\n",
    "    TN = sum(predictions == y & ~predictions)\n",
    "    FP = sum(predictions == 1 & ~y)\n",
    "    FN = sum(predictions == 0 & y)\n",
    "    \n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "print(confusion_matrix(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations and understanding weights\n",
    "\n",
    "To better understand what relations our model has found in the data we will have to take a look at the weights that our model has learned. The relative size of each weight is a direct indication of how important the corresponding feature is in defining the relationship between the input features and the target output. We can even argue why this is the case, as the hypothesis of our model is entirely dependant on the linear combination of the input values and our weights:\n",
    "\n",
    "$$ h_{\\theta}(x^i) = g(x^i_0 \\theta_0 + x^i_1 \\theta_1 + \\dots + x^i_n \\theta_n)$$\n",
    "\n",
    "The logistic function simply \"squashes\" the combined positive values to a maximum of 1, and the combined negative values to a minimum of 0. I.e. a negative weight means that a large value for this feature will actually decrease the chances of survival for a passenger. \n",
    "\n",
    "The input for our function $g$ is composed out of the sums of each input multiplied with its corresponding weight. Since we have made sure that our data all approximately has equal scale by using the `zscore` method, bigger values for a specific weight result in the input corresponding to that weight having more influence on the hypothesis. Similarly, negative values indicate that there is a negative correlation between the input variable and the hypothesis, while positive values indicate a positive correlation.\n",
    "\n",
    "Below, we have used Seaborn to create a barplot that shows each of the weights and their corresponding data column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(X_train.columns, theta[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that the three most important predictive features seem to be the Passenger Class, the Fare the passenger paid in pounds and whether the passenger was male or female. We cannot directly draw conclusions about the relationship between these features and whether or not a person survived based on this plot, but we *can* use it as a starting point for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the data\n",
    "\n",
    "First, let's take a look at a basic count plot to see how many people have survived overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Survived',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this plot that approximately 350 people have survived, while approximately 550 people have died overall.\n",
    "\n",
    "Now, lets take a look at those 3 most predictive features specifically, and see how they affect the distribution.\n",
    "\n",
    "### 1. Gender of the passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Survived',hue='Sex',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Class of the passenger's cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Survived',hue='Pclass',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fare the passenger paid\n",
    "\n",
    "This is a continous variable, so we can't plot all the the distinct cases. Instead we'll use a barplot with the averages for both classes, and corresponding error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.barplot(x='Survived', y='Fare', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating hypotheses for the data\n",
    "\n",
    "For each of the 3 features shown above, compare their individual impact on the surival chance of the passenger. For every feature, give your own hypothesis on why this feature is likely to have a positive / negative impact on the survivability of the passenger. Also, add how this then explains the positive or negative weight the feature seems to get in the learned logistic regression model:\n",
    "\n",
    "#### 1. Gender of the passenger\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "#### 2. Class of the passenger's cabin\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "#### 3. Fare the passenger paid\n",
    "\n",
    "*Your answer here.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
