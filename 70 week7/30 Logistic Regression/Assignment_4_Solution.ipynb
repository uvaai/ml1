{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are a supervised learning algorithm used for both classification and regression tasks. For this assignment we are going to take a look at classification. We can use decision trees for issues where we have numerical but also categorical input and target features. The decision tree algorithm you will be making will be based off the *ID3* algorithm as described in chapter 3 of the book \"Machine Learning\" by Tom Mitchell, sections **3.1-3.4**.\n",
    "\n",
    "After building our own *ID3* tree, we will then use scikit-learn to explore numerical univariate decision trees and some possibilities for improving them. Besides working with data containing both categorical and numeric features, this assignment will give us a chance to study the following topics:\n",
    "\n",
    "* Dealing with a more realistic data set. The Iris dataset is useful to get started, as it just works out of the box, but real world datasets will almost always be a lot messier. Having to do some preprocessing to end up with a usable representation is extremely common.\n",
    "* Working with dataframes. `pandas` is a very popular Python library for Data Science that enables you to perform database-like operations on large datasets with great performance. It is a very useful tool to add to your arsenal.\n",
    "* Analysing the results of an algorithm. The results you get when you have (correctly) implemented the algorithm might surprise you. Trying to set up hypotheses about why this is the case and what you could do to improve / prevent / fix this, is a key skill in applying machine learning on real problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "Before we start with the actual assignment, we will introduce `pandas` through a set of small exercises. For this we will use the book by the creator of `pandas`: \"Python for Data Analysis\". You won't have to read the whole book, but it will serve as a useful reference while you figure out how certain operations are done in `pandas`. We will always refer to the relevant pages, which are included in the *syllabus*.\n",
    "\n",
    "Below each of the exercises is a set of assertions that test whether you gave the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series [1 pt]\n",
    "\n",
    "Start by reading Python for Data Analysis pages 111-115 on Series.\n",
    "\n",
    "Create a `Series`-object named `earnings` containing the following *figures* and using the *sources* as its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sales            39041\n",
       "ads               8702\n",
       "subscriptions    13200\n",
       "donations          292\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earnings_sources = [\"sales\", \"ads\", \"subscriptions\", \"donations\"]\n",
    "earnings_figures = [39041, 8702, 13200, 292]\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "earnings = pd.Series(earnings_figures, index=earnings_sources)\n",
    "### END SOLUTION\n",
    "\n",
    "earnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(earnings) is pd.Series, \"Income has to be a Series\"\n",
    "assert list(earnings.index) == [\"sales\", \"ads\", \"subscriptions\", \"donations\"]\n",
    "assert list(earnings) == [39041, 8702, 13200, 292]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Series`-object named `expenses` using the same format as `earning` with the expenses *figures*. Then create another `Series`-object named `profit` with the profit figures for each category (earnings minus expenses). Lastly, create a variable `total_profit` containing the summed total of `profit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses_sources = [\"ads\", \"sales\", \"donations\", \"subscriptions\"]\n",
    "expenses_figures = [4713, 24282, 0, 3302]\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "expenses = pd.Series(expenses_figures, index=expenses_sources)\n",
    "profit = earnings - expenses\n",
    "total_profit = profit.sum()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(expenses) is pd.Series\n",
    "assert type(profit) is pd.Series\n",
    "float(total_profit)\n",
    "assert list(profit) == [3989, 292, 14759, 9898]\n",
    "assert total_profit == 28938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames [1 pt]\n",
    "\n",
    "Next read Python for Data Analysis pages 115-120 on DataFrames.\n",
    "\n",
    "Create a `DataFrame` named `skittles` with the *columns* `amount` and `rating`, using the different colors as the *index*.\n",
    "\n",
    "|&nbsp;      | amount | rating |\n",
    "|------------|--------|--------|\n",
    "| **red**    | 7      | 3      |\n",
    "| **green**  | 4      | 4      |\n",
    "| **blue**   | 6      | 2      |\n",
    "| **purple** | 5      | 4      |\n",
    "| **pink**   | 6      | 3.5    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "skittles = DataFrame([(7, 3), (4, 4), (6, 2), (5, 4), (6, 3.5)],\n",
    "                     columns=[\"amount\", \"rating\"],\n",
    "                     index=[\"red\", \"green\", \"blue\", \"purple\", \"pink\"])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(skittles) is DataFrame\n",
    "assert list(skittles.index) == [\"red\", \"green\", \"blue\", \"purple\", \"pink\"]\n",
    "assert list(skittles.columns) == [\"amount\", \"rating\"]\n",
    "assert skittles.loc[\"red\", \"amount\"] == 7\n",
    "assert skittles.loc[\"blue\", \"rating\"] == 2\n",
    "\n",
    "solution = pd.read_msgpack(b'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x92\\xa6amount\\xa6rating\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x95\\xa3red\\xa5green\\xa4blue\\xa6purple\\xa4pink\\xa8compress\\xc0\\xa6blocks\\x92\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x01\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xd7\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7(\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@\\x00\\x00\\x00\\x00\\x00\\x00\\x0c@\\xa5shape\\x92\\x01\\x05\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x01\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xd7\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7(\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa5shape\\x92\\x01\\x05\\xa5dtype\\xa5int64\\xa5klass\\xa8IntBlock\\xa8compress\\xc0')\n",
    "pd.testing.assert_frame_equal(skittles, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average _rating_ of all the skittles and store it in a variable called `skittles_average`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "skittles_average = skittles[\"rating\"].mean()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(skittles_average)\n",
    "assert skittles_average == 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column to the skittles `DataFrame` called `score`. The score of a color is equal to `amount * rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "skittles[\"score\"] = skittles[\"amount\"] * skittles[\"rating\"]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"score\" in skittles\n",
    "solution = pd.read_msgpack(b'\\x87\\xa3typ\\xa6series\\xa5klass\\xa6Series\\xa4name\\xa5score\\xa5index\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x95\\xa3red\\xa5green\\xa4blue\\xa6purple\\xa4pink\\xa8compress\\xc0\\xa5dtype\\xa7float64\\xa4data\\xc7(\\x00\\x00\\x00\\x00\\x00\\x00\\x005@\\x00\\x00\\x00\\x00\\x00\\x000@\\x00\\x00\\x00\\x00\\x00\\x00(@\\x00\\x00\\x00\\x00\\x00\\x004@\\x00\\x00\\x00\\x00\\x00\\x005@\\xa8compress\\xc0')\n",
    "pd.testing.assert_series_equal(skittles[\"score\"], solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re)Indexing [1 pt]\n",
    "\n",
    "Read Python for Data Analysis pages 122-128, Reindexing, Dropping entries, and \"Indexing, selection, and filtering\".\n",
    "\n",
    "Reindex the given `DataFrame` on columns 'a', 'c', and 'e', using indices 10, 20, 50, 60 and store the result in the same `frame` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>35.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a     c     e\n",
       "10   0.0   2.0   4.0\n",
       "20   7.0   9.0  11.0\n",
       "50  28.0  30.0  32.0\n",
       "60  35.0  37.0  39.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "frame = DataFrame(np.arange(6 * 7.).reshape((6, 7)), index=[10, 20, 30, 40, 50, 60], columns=list('abcdefg'))\n",
    "### BEGIN SOLUTION\n",
    "frame = frame.reindex(index=[10, 20, 50, 60], columns=['a', 'c', 'e'])\n",
    "### END SOLUTION\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(frame) == DataFrame\n",
    "solution = pd.read_msgpack(b'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x93\\xa1a\\xa1c\\xa1e\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xaaInt64Index\\xa4name\\xc0\\xa5dtype\\xa5int64\\xa4data\\xc7 \\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x002\\x00\\x00\\x00\\x00\\x00\\x00\\x00<\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6blocks\\x91\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x03\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xc7\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7`\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1c@\\x00\\x00\\x00\\x00\\x00\\x00<@\\x00\\x00\\x00\\x00\\x00\\x80A@\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\"@\\x00\\x00\\x00\\x00\\x00\\x00>@\\x00\\x00\\x00\\x00\\x00\\x80B@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@\\x00\\x00\\x00\\x00\\x00\\x00&@\\x00\\x00\\x00\\x00\\x00\\x00@@\\x00\\x00\\x00\\x00\\x00\\x80C@\\xa5shape\\x92\\x03\\x04\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0')\n",
    "pd.testing.assert_frame_equal(frame, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all values in the data frame that are *divisible by 3* with the value *0*, and once again store the result in the `frame` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a     b     c     d     e     f     g\n",
       "10   0.0   1.0   2.0   0.0   4.0   5.0   0.0\n",
       "20   7.0   8.0   0.0  10.0  11.0   0.0  13.0\n",
       "30  14.0   0.0  16.0  17.0   0.0  19.0  20.0\n",
       "40   0.0  22.0  23.0   0.0  25.0  26.0   0.0\n",
       "50  28.0  29.0   0.0  31.0  32.0   0.0  34.0\n",
       "60  35.0   0.0  37.0  38.0   0.0  40.0  41.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = DataFrame(np.arange(6 * 7.).reshape((6, 7)), index=[10, 20, 30, 40, 50, 60], columns=list('abcdefg'))\n",
    "### BEGIN SOLUTION\n",
    "frame[frame % 3 == 0] = 0\n",
    "### END SOLUTION\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(frame) == DataFrame\n",
    "\n",
    "solution = pd.read_msgpack(b'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x97\\xa1a\\xa1b\\xa1c\\xa1d\\xa1e\\xa1f\\xa1g\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xaaInt64Index\\xa4name\\xc0\\xa5dtype\\xa5int64\\xa4data\\xc70\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1e\\x00\\x00\\x00\\x00\\x00\\x00\\x00(\\x00\\x00\\x00\\x00\\x00\\x00\\x002\\x00\\x00\\x00\\x00\\x00\\x00\\x00<\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6blocks\\x91\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x07\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xc78\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc8\\x01P\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1c@\\x00\\x00\\x00\\x00\\x00\\x00,@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00<@\\x00\\x00\\x00\\x00\\x00\\x80A@\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00 @\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x006@\\x00\\x00\\x00\\x00\\x00\\x00=@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x000@\\x00\\x00\\x00\\x00\\x00\\x007@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80B@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00$@\\x00\\x00\\x00\\x00\\x00\\x001@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00?@\\x00\\x00\\x00\\x00\\x00\\x00C@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@\\x00\\x00\\x00\\x00\\x00\\x00&@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x009@\\x00\\x00\\x00\\x00\\x00\\x00@@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x003@\\x00\\x00\\x00\\x00\\x00\\x00:@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00D@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00*@\\x00\\x00\\x00\\x00\\x00\\x004@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00A@\\x00\\x00\\x00\\x00\\x00\\x80D@\\xa5shape\\x92\\x07\\x06\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0')\n",
    "pd.testing.assert_frame_equal(frame, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes this `pandas` introduction. Now lets move on to the actual decision tree assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting heart disease \n",
    "\n",
    "The data set we will be using for this assignment contains heart disease diagnosis results from 4 different hospitals. The data set can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/). Download the dataset, and place the files in the same folder as this notebook.\n",
    "\n",
    "Lets start by looking at the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) file, which contains a description of the data set. The file also gives an explanation for the values of the different variables, so when our tree is complete we can interpret the decision rules created by the algorithm. \n",
    "\n",
    "Some variables included here, like *#9 cp: chest pain type*, with 4 labels for different types of chest pain, are clearly categorical. Then there are variables like *#12 chol: serum cholestoral in mg/dl*, containing the concentration of cholesterol, an obvious numeric value. The ability to handle both of these types of data is something not many other machine learning algorithms can do effectively, so in theory a decision tree should be perfect for this data.\n",
    "\n",
    "### Taking a first look [2 pts]\n",
    "\n",
    "Start by downloading the 4 `processed.X.data` files and loading them into a *Pandas DataFrame* with the function `pd.read_csv`. Do not forget to manually set the names of the columns! You can find these names listed in the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) file. \n",
    "\n",
    "Create a list `data` that contains these four dataframes and print the dataset. You should see a couple of unexpected values pop up, which probably indicate a missing value. The file describing the data sets states that missing values are indicated by $-9.0$, but this doesn't seem to be the case. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>132</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>142</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>213</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>127</td>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>?</td>\n",
       "      <td>385</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp trestbps chol fbs  restecg thalach exang oldpeak slope ca  \\\n",
       "0     63    1   4      140  260   0        1     112     1       3     2  ?   \n",
       "1     44    1   4      130  209   0        1     127     0       0     ?  ?   \n",
       "2     60    1   4      132  218   0        1     140     1     1.5     3  ?   \n",
       "3     55    1   4      142  228   0        1     149     1     2.5     1  ?   \n",
       "4     66    1   3      110  213   1        2      99     1     1.3     2  ?   \n",
       "..   ...  ...  ..      ...  ...  ..      ...     ...   ...     ...   ... ..   \n",
       "195   54    0   4      127  333   1        1     154     0       0     ?  ?   \n",
       "196   62    1   1        ?  139   0        1       ?     ?       ?     ?  ?   \n",
       "197   55    1   4      122  223   1        1     100     0       0     ?  ?   \n",
       "198   58    1   4        ?  385   1        2       ?     ?       ?     ?  ?   \n",
       "199   62    1   2      120  254   0        2      93     1       0     ?  ?   \n",
       "\n",
       "    thal  num  \n",
       "0      ?    2  \n",
       "1      ?    0  \n",
       "2      ?    2  \n",
       "3      ?    1  \n",
       "4      ?    0  \n",
       "..   ...  ...  \n",
       "195    ?    1  \n",
       "196    ?    0  \n",
       "197    6    2  \n",
       "198    ?    0  \n",
       "199    ?    1  \n",
       "\n",
       "[200 rows x 14 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = []\n",
    "column_names = []\n",
    "data = []\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "file_names = ['va', 'hungarian', 'switzerland', 'cleveland']\n",
    "column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n",
    "\n",
    "data = [pd.read_csv(f'processed.{fn}.data', header=None, names=column_names) for fn in file_names]\n",
    "\n",
    "data[0]\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the scope of this problem by writing the `count_missing` function, which should count the number of missing elements for each feature / column of a data set. The function should thus return a *Series* of missing counts, one count for each column in the data set. We have provided you with the code to print some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: va\n",
      "Rows: 200\n",
      "Missing per column: \n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0    0    0   0        56     7    7        0       53     53       56    102   \n",
      "\n",
      "    ca  thal  num  \n",
      "0  198   166    0  \n",
      "\n",
      "Filename: hungarian\n",
      "Rows: 294\n",
      "Missing per column: \n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0    0    0   0         1    23    8        1        1      1        0    190   \n",
      "\n",
      "    ca  thal  num  \n",
      "0  291   266    0  \n",
      "\n",
      "Filename: switzerland\n",
      "Rows: 123\n",
      "Missing per column: \n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0    0    0   0         2     0   75        1        1      1        6     17   \n",
      "\n",
      "    ca  thal  num  \n",
      "0  118    52    0  \n",
      "\n",
      "Filename: cleveland\n",
      "Rows: 303\n",
      "Missing per column: \n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0    0    0   0         0     0    0        0        0      0        0      0   \n",
      "\n",
      "   ca  thal  num  \n",
      "0   4     2    0  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_missing(df):\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.sum(df.applymap(lambda x: x == '?'), axis=0)\n",
    "    ### END SOLUTION\n",
    "\n",
    "for i in range(len(data)):\n",
    "    print(f'Filename: {file_names[i]}')\n",
    "    print(f'Rows: {len(data[i])}')\n",
    "    \n",
    "    # The following code makes sure that we print horizontally and not vertically\n",
    "    print(f'Missing per column: \\n{count_missing(data[i]).to_frame().T}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data [1 pt]\n",
    "\n",
    "Looking at the results from the previous step, it seems like the sets from some hospitals are more complete than others. There are different approaches you might take to solve this, like replacing the missing values with the average value for that variable, or handling missing values within the algorithm in a seperate way. For now we will take the simplest approach, discarding any rows that contain missing values. This way we only use the complete patient records from each data set. \n",
    "\n",
    "We have concatenated all four DataFrames into a single DataFrame called `df`. Drop any rows containing a missing value. You can use pandas `pd.to_numeric` with keyword `errors='coerce'` to easily convert entries to floats. When trying to convert the missing values, the DataFrame will transform these into `NaN` (Not a Number) instead of a float. After you remove the rows with `NaN`, you should end up with about 300 patient records, most of which are from the Cleveland hospital. Each patient has 14 variables, one of which (the 14th variable) is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(data, ignore_index=True)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df = df.dropna()\n",
    "### END SOLUTION\n",
    "\n",
    "assert len(df) == 299\n",
    "assert len(df.columns) == 14\n",
    "\n",
    "target = df.columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at how how often each label in the target variable actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 56, 0: 160, 2: 35, 3: 35, 4: 13})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of each of the labels of our target variable differs greatly. While there are 160 labels with the value $0$, we only have 13 labels with value $4$. In virtually all classification tasks, training data that contains different numbers of representatives from each class might result in a classifier that is biased towards the most common class. When applied to a test set that is similarly imbalanced, this classifier yields an optimistic accuracy estimate. In an extreme case, the classifier might assign every single test case to the majority class, thereby trivially achieving an accuracy equal to the proportion of test cases belonging to the majority class!\n",
    "\n",
    "In the description file ([heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names)) it says *Value 0: < 50% diameter narrowing* and *Value 1: > 50% diameter narrowing*, but it does not explain all the other values. It might be logical to assume that these are different degrees of narrowing, where $0$ would mean no disease and higher values would mean different levels of disease present. Because the distribution of the different values is so skewed however, for now we will just focus on classifying the difference between a value of $0$  and any of the higher values.\n",
    "\n",
    "Change the target column to contain a boolean value that is `True` if there is more than $50\\%$ narrowing and `False` otherwise. Then print the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({True: 139, False: 160})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "df[target] = df[target] > 0\n",
    "\n",
    "Counter(df[target])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Decision Trees\n",
    "\n",
    "If you have not yet read sections **3.1-3.4** of \"Machine Learning\" by Tom Mitchell, do so now. The entire chapter 3 on Decision Trees is included in the syllabus.\n",
    "\n",
    "The main idea of decision trees is to find the feature that contains the most \"information\" and then split/group the dataset along the rows of this feature that have the same value. This process of finding the \"most informative\" feature and then splitting is repeated until we arrive at a stopping criterium.\n",
    "\n",
    "We will implement this process over several steps:\n",
    "- Generate splits from a given dataset\n",
    "- Calculate the Shannon entropy; a measure of the amount of information in a given dataset\n",
    "- Calculate the Information Gain for a given split\n",
    "- Combine these metrics into an algorithm that creates the Decision Tree\n",
    "\n",
    "## Splits [3 pts]\n",
    "\n",
    "Before we can determine which of our features produce the most descriptive split, we must create each split. **A split is a grouping of rows in a dataset by each of the unique values in _one_ of the columns.**\n",
    "\n",
    "First, we need to determine what unique values are present in a column. Implement a function named `unique_values` that takes a dataframe `df` and a column name `m` and returns a list of the unique values in the given column. Test the function by entering a couple of the column names and see if the outcome is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29. 34. 35. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51.\n",
      " 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69.\n",
      " 70. 71. 74. 76. 77.]\n"
     ]
    }
   ],
   "source": [
    "def unique_values(df, m):\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.unique(df[m])\n",
    "    ### END SOLUTION\n",
    "\n",
    "print(unique_values(df, 'age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement a function named `create_split` that takes a dataframe `df` and a column name `m` and returns a dictionary. This dictionary has the *unique column values* as its keys, and the split dataframes as its values. Remember that you just wrote a function for getting the unique keys! Each of the dataframes consist of only data rows that match with the same unique value. Simply put, the data frame rows are \"grouped\" by each of the different unique values from that column.\n",
    "\n",
    "*Note:* When adding each of the frames in the dictionary, remove the column that was used to create the split. We have already split it into its different unique values, so the information in that column is now redundant and further splits on this column would not be possible.\n",
    "\n",
    "You should end up with a dictionary that has a set of rows for each unique value in the given column. Test the function by entering a column name, and see if the outcome is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-55280b5b72f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m### END SOLUTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msolution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_msgpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x9d\\xa3sex\\xa2cp\\xa8trestbps\\xa4chol\\xa3fbs\\xa7restecg\\xa7thalach\\xa5exang\\xa7oldpeak\\xa5slope\\xa2ca\\xa4thal\\xa3num\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xaaInt64Index\\xa4name\\xc0\\xa5dtype\\xa5int64\\xa4data\\xd8\\x00\\xce\\x02\\x00\\x00\\x00\\x00\\x00\\x00J\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6blocks\\x92\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x0c\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xc7`\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x80]@\\x00\\x00\\x00\\x00\\x00\\x80]@\\x00\\x00\\x00\\x00\\x00\\xc0f@\\x00\\x00\\x00\\x00\\x00@j@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0e@\\x00\\x00\\x00\\x00\\x00\\x00h@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00ffffff\\xe6?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\xa5shape\\x92\\x0c\\x02\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x01\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xd7\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xd5\\x00\\x00\\x00\\xa5shape\\x92\\x01\\x02\\xa5dtype\\xa4bool\\xa5klass\\xa9BoolBlock\\xa8compress\\xc0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_frame_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'age'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m34\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def create_split(df, m):\n",
    "    ### BEGIN SOLUTION\n",
    "    uvs = unique_values(df, m)\n",
    "    return {uv: df[df[m] == uv].drop(m, axis=1) for uv in uvs}\n",
    "    ### END SOLUTION\n",
    "\n",
    "solution = pd.read_msgpack(b'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x9d\\xa3sex\\xa2cp\\xa8trestbps\\xa4chol\\xa3fbs\\xa7restecg\\xa7thalach\\xa5exang\\xa7oldpeak\\xa5slope\\xa2ca\\xa4thal\\xa3num\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xaaInt64Index\\xa4name\\xc0\\xa5dtype\\xa5int64\\xa4data\\xd8\\x00\\xce\\x02\\x00\\x00\\x00\\x00\\x00\\x00J\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6blocks\\x92\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x0c\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xc7`\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x80]@\\x00\\x00\\x00\\x00\\x00\\x80]@\\x00\\x00\\x00\\x00\\x00\\xc0f@\\x00\\x00\\x00\\x00\\x00@j@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0e@\\x00\\x00\\x00\\x00\\x00\\x00h@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00ffffff\\xe6?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\xa5shape\\x92\\x0c\\x02\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x01\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xd7\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xd5\\x00\\x00\\x00\\xa5shape\\x92\\x01\\x02\\xa5dtype\\xa4bool\\xa5klass\\xa9BoolBlock\\xa8compress\\xc0')\n",
    "pd.testing.assert_frame_equal(create_split(df, 'age')[34], solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy [3 pts]\n",
    "There quite a few different definitions of what entropy is; all of them relate to the notion of chaos / order in a system, but the exact definition strongly depends on the context in which the term is used. Most commonly the term refers to thermodynamic entropy, where it is the describes the number possible configurations a thermodynamic system can have in a specific state. This is related to the idea of a universal entropy, as used in Asimov's classic short story [The Last Question](http://multivax.com/last_question.html). For decision trees we need the information theoretic entropy, or Shannon entropy, which says something about the amount of information contained in a distribution of data. The more ordered or one-sided the distribution is, the less bits we would need on average to express the exact distribution.\n",
    "\n",
    "We will use this measure of entropy to compare the results of decision tree splits to see which is the \"most informative\". For the heart disease problem there are now only 2 class labels we are considering, `True` if *vascular narrowing >= 50% diameter* and `False` otherwise. For a 2 class problem, the entropy is defined as:\n",
    "\n",
    "(3.1) $$\\phi(p) = −p\\ log_2(p) − (1 − p)\\ log_2(1 − p)$$\n",
    "\n",
    "where $p$ is the ratio between between the labels for class 1 and class 2. \n",
    "\n",
    "First, write a `ratio` function to compute $p$. The function should, given a list of boolean values as class labels, return the ratio of `True` labels in the list, e.g. $1.0$ would indicate the list only contained `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(labels):\n",
    "    ### BEGIN SOLUTION\n",
    "    return sum(labels) / len(labels)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets assume that $p=0$; meaning there are no labels that equal `True`. The computation of $-0\\ log_2(0)$ would then (correctly) result in a math error, however this could also just be defined as having the value $0$ (as it is multiplied by $0$). \n",
    "\n",
    "Write the function `entropy_sub` to compute the value of the log product ($-p\\ log_2(p)$), making sure to return $0$ in the case that $p = 0$. Combine `ratio` and `entropy_sub` to compute the `entropy` of a list of boolean class labels (equation 3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy_sub(p):\n",
    "    ### BEGIN SOLUTION\n",
    "    return -1 * p * math.log(p, 2) if p != 0 else 0\n",
    "    ### END SOLUTION\n",
    "\n",
    "def entropy(labels):\n",
    "    ### BEGIN SOLUTION\n",
    "    p = ratio(labels)\n",
    "    return entropy_sub(p) + entropy_sub(1 - p)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `plot_entropy` function to check whether the entropy function works correctly. This function should create many different lists of boolean labels of length N and compute ratio and entropy for each of these lists. Note that for a list of booleans of length 𝑁, there are only 𝑁+1 different possible ratios of labels you need to create. The x-axis of your plot should show the ratios and the y-axis their resulting entropies, which should produce a graph like Figure 3.2 in Mitchell. Show this plot at the end of your code and make sure it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_entropy(N):\n",
    "    ### BEGIN SOLUTION\n",
    "    rat, ent = [], []\n",
    "    \n",
    "    for i in range(N + 1):\n",
    "        labels = i * [True] + (N - i) * [False]\n",
    "        rat.append(ratio(labels))\n",
    "        ent.append(entropy(labels))\n",
    "        \n",
    "    plt.plot(rat, ent)\n",
    "    plt.show()\n",
    "    \n",
    "plot_entropy(50)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain [3 pts]\n",
    "There are several metrics we can use to asses how good a split is in a decision tree. For this implemenation we will use the *Information Gain*, which is defined as the entropy of the original distribution $\\phi(p)$, minus the entropy of the split distribution $I_m$ resulting from the split on variable $m$.\n",
    "\n",
    "(3.4a) $$IG_m = \\phi(p) - I_m$$\n",
    "\n",
    "Information Gain measures how much the entropy changes from making a specific split, i.e. the gain in predictablity of the data as a result of making a distribution based on a specific variable. When a set of target labels is split on a variable $m$, two or more new lists are created, each with their own entropy. Combining the resulting entropies from a split into $s$ new sets is a simple weighted sum:\n",
    "\n",
    "(3.4b) $$I_m = \\sum_{j=1}^s \\frac{N_j}{N} \\phi(p_j)$$\n",
    "\n",
    "where $N_j$ is the size of the $j^{th}$ split distribution, $p_j$ is the ratio of the target labels for that same $j^{th}$ distribution and $N$ is the size of the distribution before the split. \n",
    "\n",
    "Write the function `split_entropy` to compute $I_m$ for some list of target labels and a given `N`. The `split_labels` argument is a list containing $s$ different lists, each containing the target labels for one part of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.603759374819711\n"
     ]
    }
   ],
   "source": [
    "def split_entropy(split_labels, N):\n",
    "    ### BEGIN SOLUTION\n",
    "    return sum([len(labels) / N * entropy(labels) for labels in split_labels])\n",
    "    ### END SOLUTION\n",
    "print(split_entropy([[True,False,False,True],[False,False,False,False],[True,False,False,False]],12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, our function `create_split` returns a dictionary where the keys are the unique entries present in the column chosen and the values are dataframes that contain the rows that have those unique entries. However, the function `split_entropy` expects a list wherein each element is a list of target labels.\n",
    "\n",
    "Write the function `get_split_labels` that accepts a `split` dictionary and `target` (the name of the column that contains the labels) and returns the `split_labels`. This returned list of lists should be of the same format as before, so it can be used directly as input for the `split_entropy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[748    False\n",
      "839    False\n",
      "Name: num, dtype: bool, 739    False\n",
      "780    False\n",
      "812     True\n",
      "863     True\n",
      "Name: num, dtype: bool, 777    False\n",
      "Name: num, dtype: bool, 766    False\n",
      "768    False\n",
      "Name: num, dtype: bool, 697    False\n",
      "Name: num, dtype: bool, 667    False\n",
      "761    False\n",
      "833    False\n",
      "Name: num, dtype: bool, 873    False\n",
      "Name: num, dtype: bool, 710    False\n",
      "722    False\n",
      "762     True\n",
      "793    False\n",
      "838    False\n",
      "841     True\n",
      "Name: num, dtype: bool, 633     True\n",
      "637    False\n",
      "646     True\n",
      "663    False\n",
      "674     True\n",
      "690     True\n",
      "691     True\n",
      "713     True\n",
      "720    False\n",
      "744     True\n",
      "779    False\n",
      "817    False\n",
      "821    False\n",
      "845     True\n",
      "857    False\n",
      "864     True\n",
      "867    False\n",
      "897     True\n",
      "915     True\n",
      "Name: num, dtype: bool, 662     True\n",
      "669     True\n",
      "764    False\n",
      "786    False\n",
      "831     True\n",
      "837    False\n",
      "843    False\n",
      "846     True\n",
      "890    False\n",
      "Name: num, dtype: bool, 902    True\n",
      "Name: num, dtype: bool, 717    False\n",
      "769    False\n",
      "871    False\n",
      "Name: num, dtype: bool, 648    True\n",
      "Name: num, dtype: bool, 718    False\n",
      "726     True\n",
      "729    False\n",
      "776    False\n",
      "842    False\n",
      "849     True\n",
      "877    False\n",
      "Name: num, dtype: bool, 28      True\n",
      "619     True\n",
      "622    False\n",
      "624    False\n",
      "630    False\n",
      "639     True\n",
      "642    False\n",
      "643    False\n",
      "653     True\n",
      "668    False\n",
      "681     True\n",
      "689     True\n",
      "701    False\n",
      "707    False\n",
      "721     True\n",
      "725     True\n",
      "745    False\n",
      "747    False\n",
      "754     True\n",
      "755     True\n",
      "771     True\n",
      "803    False\n",
      "811    False\n",
      "815    False\n",
      "827    False\n",
      "828     True\n",
      "832    False\n",
      "850    False\n",
      "854     True\n",
      "856    False\n",
      "861    False\n",
      "862     True\n",
      "870    False\n",
      "872    False\n",
      "880    False\n",
      "906    False\n",
      "909     True\n",
      "912    False\n",
      "Name: num, dtype: bool, 716    False\n",
      "751    False\n",
      "852     True\n",
      "900    False\n",
      "Name: num, dtype: bool, 840    True\n",
      "Name: num, dtype: bool, 672     True\n",
      "746    False\n",
      "781    False\n",
      "797     True\n",
      "876     True\n",
      "911     True\n",
      "Name: num, dtype: bool, 675    False\n",
      "676    False\n",
      "688     True\n",
      "693     True\n",
      "728     True\n",
      "756    False\n",
      "760     True\n",
      "774     True\n",
      "778     True\n",
      "865     True\n",
      "Name: num, dtype: bool, 785     True\n",
      "858    False\n",
      "884     True\n",
      "Name: num, dtype: bool, 679     True\n",
      "712     True\n",
      "719    False\n",
      "724     True\n",
      "759    False\n",
      "765    False\n",
      "823     True\n",
      "866    False\n",
      "869    False\n",
      "899     True\n",
      "Name: num, dtype: bool, 807    False\n",
      "Name: num, dtype: bool, 620    False\n",
      "621    False\n",
      "625     True\n",
      "629     True\n",
      "635    False\n",
      "636    False\n",
      "641     True\n",
      "651    False\n",
      "661     True\n",
      "666    False\n",
      "670    False\n",
      "671     True\n",
      "677     True\n",
      "695    False\n",
      "698    False\n",
      "706    False\n",
      "709    False\n",
      "731     True\n",
      "735     True\n",
      "737     True\n",
      "742    False\n",
      "749    False\n",
      "772     True\n",
      "795    False\n",
      "796    False\n",
      "825    False\n",
      "829    False\n",
      "835    False\n",
      "853     True\n",
      "859    False\n",
      "886    False\n",
      "896    False\n",
      "898    False\n",
      "905    False\n",
      "917     True\n",
      "918     True\n",
      "Name: num, dtype: bool, 640     True\n",
      "655     True\n",
      "730     True\n",
      "782    False\n",
      "784    False\n",
      "794     True\n",
      "908    False\n",
      "Name: num, dtype: bool, 715    False\n",
      "798     True\n",
      "855    False\n",
      "860     True\n",
      "891     True\n",
      "Name: num, dtype: bool, 650    False\n",
      "680    False\n",
      "711    False\n",
      "732    False\n",
      "736     True\n",
      "752    False\n",
      "Name: num, dtype: bool, 847    False\n",
      "878     True\n",
      "882     True\n",
      "Name: num, dtype: bool, 703    False\n",
      "705    False\n",
      "734    False\n",
      "741     True\n",
      "810     True\n",
      "814    False\n",
      "834    False\n",
      "836    False\n",
      "881     True\n",
      "894    False\n",
      "Name: num, dtype: bool, 623     True\n",
      "626     True\n",
      "627    False\n",
      "628    False\n",
      "634    False\n",
      "647    False\n",
      "649     True\n",
      "652    False\n",
      "658    False\n",
      "665    False\n",
      "673     True\n",
      "683     True\n",
      "694    False\n",
      "699    False\n",
      "702    False\n",
      "723     True\n",
      "733    False\n",
      "740     True\n",
      "750    False\n",
      "757    False\n",
      "773     True\n",
      "775     True\n",
      "790    False\n",
      "802    False\n",
      "806     True\n",
      "808     True\n",
      "820    False\n",
      "874    False\n",
      "887     True\n",
      "889     True\n",
      "910     True\n",
      "914     True\n",
      "Name: num, dtype: bool, 678    False\n",
      "788    False\n",
      "822     True\n",
      "Name: num, dtype: bool, 824    True\n",
      "916    True\n",
      "Name: num, dtype: bool, 617    False\n",
      "682     True\n",
      "727     True\n",
      "753     True\n",
      "791     True\n",
      "Name: num, dtype: bool, 868     True\n",
      "893    False\n",
      "Name: num, dtype: bool, 799    False\n",
      "901     True\n",
      "Name: num, dtype: bool, 405     True\n",
      "632    False\n",
      "638    False\n",
      "644    False\n",
      "645    False\n",
      "654     True\n",
      "656    False\n",
      "657     True\n",
      "660    False\n",
      "664     True\n",
      "684    False\n",
      "686     True\n",
      "696     True\n",
      "714     True\n",
      "738     True\n",
      "819    False\n",
      "826     True\n",
      "879    False\n",
      "Name: num, dtype: bool, 767    False\n",
      "792     True\n",
      "844    False\n",
      "885     True\n",
      "907     True\n",
      "Name: num, dtype: bool, 895    True\n",
      "Name: num, dtype: bool, 687    False\n",
      "Name: num, dtype: bool, 875    False\n",
      "Name: num, dtype: bool, 801    True\n",
      "Name: num, dtype: bool, 618     True\n",
      "659    False\n",
      "692    False\n",
      "708     True\n",
      "770     True\n",
      "787     True\n",
      "804     True\n",
      "813    False\n",
      "816     True\n",
      "851    False\n",
      "888    False\n",
      "Name: num, dtype: bool, 913    True\n",
      "Name: num, dtype: bool, 763    True\n",
      "Name: num, dtype: bool, 685     True\n",
      "758     True\n",
      "892    False\n",
      "903     True\n",
      "Name: num, dtype: bool, 631    False\n",
      "Name: num, dtype: bool, 789    True\n",
      "Name: num, dtype: bool, 800    False\n",
      "830     True\n",
      "Name: num, dtype: bool, 700     True\n",
      "818    False\n",
      "848     True\n",
      "Name: num, dtype: bool, 805    True\n",
      "Name: num, dtype: bool, 743    True\n",
      "Name: num, dtype: bool]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "def get_split_labels(split, target):\n",
    "    ### BEGIN SOLUTION\n",
    "    return [df[target] for df in split.values()]\n",
    "    ### END SOLUTION\n",
    "    \n",
    "print(len(unique_values(df, 'trestbps')))\n",
    "print(get_split_labels(create_split(df,'trestbps'),target))\n",
    "print(len(get_split_labels(create_split(df,'trestbps'),target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the `information_gain` function using your earlier functions `entropy` and `split_entropy`. Assume `split_labels` is a list containing $s$ different lists, each containing the target labels from one of each  of the splits. Remember that one split exists of all of the grouped unique values in a column of our data, thus, the total number of target labels given to this function is $N$ and the combination of all labels is $p$. You can use `pd.concat` to \"glue\" the different Series-objects together to compute $p$ directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14199346662874135"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def information_gain(split_labels):\n",
    "    ### BEGIN SOLUTION\n",
    "    p = pd.concat(split_labels)\n",
    "    N = len(p)\n",
    "    \n",
    "    return entropy(p) - split_entropy(split_labels, N)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "information_gain(get_split_labels(create_split(df,'trestbps'),target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have created two assertions that check whether your functions were implemented correctly. \n",
    "\n",
    "**Explain why these assertions work.**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996438783594274\n",
      "0.996438783594274\n"
     ]
    }
   ],
   "source": [
    "# We create a copy, because we remove the target in create_split, while it is needed for split_labels\n",
    "test_df = df.copy()\n",
    "target_copy = target + '_copy'\n",
    "test_df[target_copy] = test_df[target]\n",
    "\n",
    "split = create_split(test_df, target_copy)\n",
    "split_labels = get_split_labels(split, target)\n",
    "\n",
    "print(entropy(test_df[target].tolist()))\n",
    "print(information_gain(split_labels))\n",
    "\n",
    "assert entropy(test_df[target].tolist()) == information_gain(split_labels)\n",
    "assert split_entropy(split_labels, len(test_df)) == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3\n",
    "\n",
    "So far, we have determined the total purity of the dataset (entropy), and created the functions that enable us to determine what column of our data is the most informative (Information Gain). That leads us to the introduction of the *ID3* algorithm introduced by Ross Quinlan in 1986. This algorithm can be used to create a categorical decision tree through a process of greedy iterations. Reread table 3.1 in Mitchell's 'Machine Learning', containing the pseudo-code for the algorithm.\n",
    "\n",
    "Our version of the *ID3* algorithm can be described as follows:\n",
    "\n",
    "1. *If the current dataset is pure (all rows have the same label), return the pure label.* Use the `unique_value` function for this.\n",
    "2. *If there are no more possible columns to split; return the most common label.*\n",
    "3. *Calculate the Information Gain of each possible split in the dataset.* First create splits for each of the columns in the dataset (except for the target column), then extract the label lists for each of these splits and use these to calculate the Information Gain of that split.\n",
    "4. *Pick the split with the largest Information Gain.*\n",
    "5. *Create a Node for this new/current branch of the tree.*\n",
    "6. *For each of the sub-datasets in the split, create a sub-tree (recursively call `ID3` with the sub-dataset) and use the sub-tree result as the branches of the tree created in step 5.*\n",
    "7. *Return the complete tree.*\n",
    "\n",
    "We have also provided you with a [namedtuple](https://docs.python.org/3/library/collections.html#collections.namedtuple) `Node` that can hold the name of the selected split column; `column_name`, the most common label in this node; `mode`, and a dictionary mapping to results of the split; `branches`, wherein the key is each of the 'unique values' possible, and the values are the result of the algorithm on the data available for that unique value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Node = namedtuple('Node', ['column_name',  'mode', 'branches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Lets take a look at an example before we start writing out the complete algorithm. Say we have the following data set and want to classify on the target column *Vegetable*:\n",
    "\n",
    "|&nbsp;         | Color  | Pits  | Vegetable |\n",
    "|---------------|--------|-------|-----------|\n",
    "| **Broccoli**  | Green  | False | True      |\n",
    "| **Pear**      | Green  | True  | False     |\n",
    "| **Strawberry**| Red    | False | False     |\n",
    "| **Zucchini**  | Green  | False | True      |\n",
    "| **Orange**    | Orange | True  | False     |\n",
    "\n",
    "\n",
    "We won't bother with computing the exact Information Gain here, but looking at the 2 possible columns and the distribution of the target column *Vegetable*, you might conclude that *Color* would be a good variable to split on.\n",
    "\n",
    "We can use the [*namedtuple*](https://docs.python.org/3/library/collections.html#collections.namedtuple) to create a `Node` in our Decision Tree and have it store all the relevant information at that level of the tree. The `column_name` to split on has already been chosen to be *Color* and the `mode` is the most common label of the target column in this node, so this would be *False*, as there are currently more fruits than vegetables in our data set. The `branches` would start out by just being the result of the `create_split` function, meaning there would be 3 new data frames, one for each *Color*. The whole `Node` would then look something like \n",
    "\n",
    "    Node(column_name='Color', mode=False, branches={\n",
    "    \n",
    "            'Green': \n",
    "\n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Broccoli**  | False | True      |\n",
    "| **Pear**      | True  | False     |\n",
    "| **Zucchini**  | False | True      |\n",
    "\n",
    "            'Orange':\n",
    "            \n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Orange**    | True  | False     |\n",
    "\n",
    "            'Red':\n",
    "\n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Strawberry**| False | False     |\n",
    "            \n",
    "            })\n",
    "    \n",
    "However, this is of course only 1 split of our data. The crucial step in the *ID3* algorithm is step **6**; recursively create the sub-trees. So we want to split each of these data sets again and if those produce new branches, we split those further *again*, etc. This is what creates the actual tree structure, the recursive splitting into nodes, each with new branches.\n",
    "\n",
    "As you hopefully remember from your first year, all recursions need base cases and that is what steps **1 and 2** are for. The *leaf nodes* are reached when we've already perfectly split the data or no more splits are possible, and in those cases we just return the predicted label. The whole purpose of the tree structure is to predict a label for the target column. So when we reach the end of some path in the Decision Tree, all we really care about is what the predicted target label would be at that leaf in the tree, and so that is the only information we will need to store there.\n",
    "\n",
    "Combining these steps, *any* branch in a `Node` should always point to another `Node` or the predicted label for that point in the tree. This means the example structure above, containing 3 data frames, would never actually be returned by the algorithm, as the recursive step to split each frame down to *leaf nodes* is still missing. The result of that would look something like\n",
    "\n",
    "    Node(column='Color', mode=False, branches={\n",
    "    \n",
    "        'Green': Node(column='Pits', mode=True, branches={\n",
    "                \n",
    "                    False: True, \n",
    "\n",
    "                    True: False}),\n",
    "\n",
    "        'Orange': False,\n",
    "        \n",
    "        'Red': False})\n",
    "\n",
    "Note that each terminal value (i.e. leaf node) is now either *True* or *False*, indicating the predicted label for the *Vegetable* column at that point in the tree. If we try and interpret this tree, there is only 1 case where it would predict that something is indeed a vegetable; when it is both *Green* and has *No Pits*. In all other cases this tree would predict it to be a fruit. Verify both the constructed tree and the resulting interpreted rule before moving on to implementation.\n",
    "\n",
    "### Implementation [7 pts]\n",
    "\n",
    "Now that we know what the tree structure will look like, lets build the *ID3* algorithm. So far, we have already included step 5 and 7 for you. Step 5 expects the variables `column_name` and `mode` to already be defined by you, in order to create the `Node`, while the branches start out empty.\n",
    "\n",
    "Implement the other steps and see if your code produces that same tree on `test_df`, which contains the vegetable example. Note that the *white space* was added to make the example more readable, so your actual result should look like:\n",
    "\n",
    "    Node(column_name='Color', mode=False, branches={'Green': Node(column_name='Pits', mode=True, branches={False: True, True: False}), 'Orange': False, 'Red': False})\n",
    "\n",
    "For your convenience we will repeat the steps of the _ID3_ algorithm (this is the same text as you can find above):\n",
    "1. *If the current dataset is pure (all rows have the same label), return the pure label.* Use the `unique_value` function for this.\n",
    "2. *If there are no more possible columns to split; return the most common label.* HINT: You can use pandas [`mode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html).\n",
    "3. *Calculate the Information Gain of each possible split in the dataset.* First create splits for each of the columns in the dataset (except for the target column), then extract the label lists for each of these splits and use these to calculate the Information Gain of that split.\n",
    "4. *Pick the split with the largest Information Gain.*\n",
    "5. *Create a Node for this new/current branch of the tree.*\n",
    "6. *For each of the sub-datasets in the split, create a sub-tree (recursively call `ID3` with the sub-dataset) and use the sub-tree result as the branches of the tree created in step 5.*\n",
    "7. *Return the complete tree.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node(column_name='Color', mode=False, branches={'Green': Node(column_name='Pits', mode=True, branches={False: True, True: False}), 'Orange': False, 'Red': False})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df = pd.DataFrame([['Green', False, True], ['Green', True, False], ['Red', False, False],\n",
    "                        ['Green', False, True], ['Orange', True, False]],\n",
    "                       index=['Broccoli', 'Pear', 'Strawberry', 'Zucchini', 'Orange'],\n",
    "                       columns=['Color', 'Pits', 'Vegetable'])\n",
    "                       \n",
    "test_target = 'Vegetable'\n",
    "\n",
    "def ID3(data, target):    \n",
    "    # step 1\n",
    "    ### BEGIN SOLUTION\n",
    "    uv = unique_values(data, target)\n",
    "    if len(uv) == 1:\n",
    "        return uv[0]\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # step 2\n",
    "    ### BEGIN SOLUTION\n",
    "    mode = data[target].mode()[0]\n",
    "    if len(data.columns) == 1:\n",
    "        return mode\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # step 3\n",
    "    ### BEGIN SOLUTION\n",
    "    splits = [create_split(data, cn) for cn in data.drop(target, axis=1)]\n",
    "    all_split_labels = [get_split_labels(split, target) for split in splits]\n",
    "    IGs = [information_gain(split_labels) for split_labels in all_split_labels]\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # step 4\n",
    "    ### BEGIN SOLUTION\n",
    "    best_split_index = np.argmax(IGs)\n",
    "    best_split = splits[best_split_index]\n",
    "    column_name = data.columns[best_split_index]\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # step 5\n",
    "    branches = {}\n",
    "    tree = Node(column_name, mode, branches)\n",
    "    \n",
    "    # step 6\n",
    "    ### BEGIN SOLUTION\n",
    "    for uv, subset in best_split.items():\n",
    "        subtree = ID3(subset, target)\n",
    "        \n",
    "        branches[uv] = subtree\n",
    "    ### END SOLUTION  \n",
    "\n",
    "    # step 7\n",
    "    return tree\n",
    "\n",
    "tree = ID3(test_df, test_target)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have built a tree, it is important to realise that this algorithm *does not* search the entire space of possible trees for the best possible decision tree, but instead opts for the step at each node that will make the most progress at that moment. The tree that results from these locally optimal choices might not be globally optimal (although that happens to be the case for this small toy example).\n",
    "\n",
    "## Classify [2 pts]\n",
    "\n",
    "With this complete tree classifying a new data-entry is pretty easy! Implement the `classify` function, which accepts a tree (a variable containing a `Node` representing the root of the tree), and a *single row* of a DataFrame, i.e. a Series. The function should return the predicted target label for that one row of data.\n",
    "\n",
    "Continuing the example from before; we have a new element from that same data set, but we don't yet know if it would be considered a vegetable, so we want to try and use our decision tree to predict if it is, e.g.\n",
    "\n",
    "|&nbsp;         | Color  | Pits  |\n",
    "|---------------|--------|-------|\n",
    "| **Tomato**    | Red    | False |\n",
    "\n",
    "The function should move through the relevant branches down the tree based on the `column_name` at each node and the corresponding value of the row we are trying to predict. There are 3 options at each branch:\n",
    "\n",
    "1. There is another Node attached to this branch, meaning we should continue further down the tree.\n",
    "2. This branch leads to a *leaf node*, meaning we now have a predicted label for the row we can return.\n",
    "3. The value from the column in our row actually does not have corresponding branch in this node. In that case the function should return the most common label in that node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, row):\n",
    "    ### BEGIN SOLUTION\n",
    "    while type(tree) is Node:\n",
    "        value = row[tree.column_name]\n",
    "        tree = tree.branches.get(value, tree.mode)\n",
    "        \n",
    "    return tree\n",
    "    ### END SOLUTION\n",
    "\n",
    "test_tomato = pd.Series(['Red', False], index=['Color', 'Pits'])\n",
    "assert not classify(tree, test_tomato), \"Tomato is a fruit!\"\n",
    "\n",
    "test_grape = pd.Series(['Blue', True], index=['Color', 'Pits'])\n",
    "assert not classify(tree, test_grape), \"Grape is a fruit!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 3rd option can occur as it is possible the categorical value that is being classified was not present in the training set at that point in the tree. This means that even though there are more branches further down the tree, none of them match the row we are trying to classify. Therefore, the best prediction we can possible give for that row, is simply the most common target label that was present in the training set at that node in the tree.\n",
    "\n",
    "It could be the case that we have never seen this categorical value in our training set before at all, as with the grape example above. However, it is also possible that a specific categorical value *did* occur in our training set, but still some nodes *do not* have a branch for that value.\n",
    "\n",
    "**Explain when this might occur and why returning the most common target label is still a valid solution.**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation [4 pts]\n",
    "\n",
    "With all these elements complete, we can start applying the *ID3* algorithm on the heart disease data set and see how well it can predict the *Vascular narrowing* based on the other features describing each patient.\n",
    "\n",
    "Before we see the accuracy of our Decision Tree, we need to create a validation split. Split the DataFrame into `train` and `test` using a ratio of $0.7$ with [sklearn's `train_test_split` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to split the train and test DataFrames into categorical and numerical features, using `cat_num_split`. This could be done manually by taking a good look at the description file ([heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names)), but it can also be done by counting the amount of unique values present in a column. As an rule of thumb, it is probably unlikely there are more than 10 different categorical values for any one column, while it is seems very likely that there are more than 10 different numerical values for a column present in the data.\n",
    "\n",
    "Use the `unique_values` function from earlier and select the variables that have no more than the `threshold` argument number of unique values as categorical variables and all others as numerical variables. The categorical variables are all whole numbers, so the resulting 2d-array should be of type `int` and the numeric variables array should of type `float`, for which you can use pandas `astype` method. Remember that both dataframes should have the target column.\n",
    "\n",
    "Apply this function to separate out the categorical and numerical features for both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_num_split(df, target, threshold=10):\n",
    "    ### BEGIN SOLUTION\n",
    "    cat_columns = [len(unique_values(df, cn)) < threshold for cn in df.columns]\n",
    "    \n",
    "    cat = df.loc[:, cat_columns].astype(int)\n",
    "    cat[target] = df[target]\n",
    "    \n",
    "    num = df.loc[:, np.logical_not(cat_columns)].astype(float)\n",
    "    num[target] = df[target]\n",
    "    \n",
    "    return cat, num\n",
    "    ### END SOLUTION\n",
    "\n",
    "train_cat, train_num = cat_num_split(train, target)\n",
    "test_cat, test_num = cat_num_split(test, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write the function `validate` that takes a decision tree, a dataframe of testing sample, and the target column name. It should `classify` all rows in the dataframe using the decision tree and return the percentage of elements that was classified correctly. \n",
    "\n",
    "\n",
    "Create a decision tree fitted to the categorical training data created above. Validate the results by computing both the train and test accuracy using the categorical features of the heart disease data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9330143540669856\n",
      "Test accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "def validate(tree, df, target):\n",
    "    ### BEGIN SOLUTION\n",
    "    return sum(df.apply(lambda x: classify(tree, x), axis=1) == df[target]) / len(df)\n",
    "\n",
    "tree = ID3(train_cat, target)\n",
    "\n",
    "print(f'Train accuracy: {validate(tree, train_cat, target)}')\n",
    "print(f'Test accuracy: {validate(tree, test_cat, target)}')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Decision Tree [2 pts]\n",
    "\n",
    "Now that our Categorical Decision Tree is done, we will take a look at a Numerical Decision tree. Start by reading section **3.7.2** from Mitchell. As stated there, it is possible to extend the Categorical Decision Tree to also include numerical boundaries. We could then use this to even select the best Information Gain from *both* the categorical *and* the numerical splits at each node.\n",
    "\n",
    "However, for now we will focus a model that can *only* make numerical splits, as that is what the `scikit-learn` library provides straight out of the box: The [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) is a class that can build a decision tree from numerical data, and so we won't have to implement this tree from scratch.\n",
    "\n",
    "The most important difference between categorical and numerical decision trees is the way the data is split. In categorical data, it is easy to split the tree into the $N$ categories that are present in the data. This is impossible to do in numerical data, as there are infinitely many categories. Categorical Decision trees therefore use binary splits or a so called split boundary; where with each split we create one branch with values smaller than the boundary, and one branch with values greater than or equal to the boundary.\n",
    "\n",
    "This boundary for a column can be determined by trying every possible split boundary available for the set of values. This is done by first sorting the samples and then trying every split half way between two neighbouring values that have different labels. This means there can be as many splits as there are samples, and as such this method is computationally very expensive. An alternative, simpler method that is often used, is trying some amount of random splits for a column and picking the best random split among them.\n",
    "\n",
    "Note that repeated binary splits on the same numerical variable be used to create many different \"decision regions\" for the same variable. For example, consider a label where you want a variable to be above or equal to 3.4, but below 4.8. Here you would need two splits; a first split with 3.4 as the boundary and then in the \"greater equal branch\" of that split, another split boundary of 4.8 on that same variable. This means that while the feature that was split on could be ignored in further splits in the case of a Categorical Decision tree, this is not the case for Numerical Decision Trees, and repeated splits on the same variable can actually greatly improve accuracy.\n",
    "\n",
    "Implement a Numerical Tree Classifier using a `sklearn.tree.DecisionTreeClassifier` and its `fit` and `predict` functions.  Train it using the numerical part of the dataset you split out earlier, and print the training and test accuracies (this can easily be done through the `metrics.accuracy_score` method from the `metrics` module). Note that the `fit` function of the classifier requires you to separate the target column from the dataframe and provide the training set as `X` and `y`; you can use `.drop(target, axis=1)` for this, as it returns a new dataframe, but without the target class column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(train_num.drop(target, axis=1), train_num[target])\n",
    "\n",
    "y_pred = clf.predict(train_num.drop(target, axis=1))\n",
    "print(f'Train accuracy: {metrics.accuracy_score(y_pred, train_num[target])}')\n",
    "\n",
    "y_pred = clf.predict(test_num.drop(target, axis=1))\n",
    "print(f'Test accuracy: {metrics.accuracy_score(y_pred, test_num[target])}')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn Decision Trees implemenation is specifically intended for numerical features. This might not be immideately obvious, but if we read the documentation on [Decision Trees](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) and see what algorithms it uses, we find:\n",
    "\n",
    "```scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.```\n",
    "\n",
    "Even if it does not actually support catagorical data, you can *still* make this implementation work, provided that you give each category a number. **Note: This is not recommended (discouraged even), as this suggests that there is an ordering (the numerical order) to the categorical data.** For many models, saying category 1 is before category 3, and category 2 is somewhere in between them, can really screw up your predictions.\n",
    "\n",
    "Even though this is not recommended, since our data already has integer numbers for each of the categories we could easily try and see what works. Train a Numerical Tree Classifier using `sklearn.DecisionTreeClassifier` using the categorical data, and print the training and test accuracies. Next, train using the complete training data, i.e. categorical and numerical combined and also report the training and testing accuracies.\n",
    "\n",
    "*Note: the linked page on Decision Trees contains more useful information and hints if you plan to use this model on any projects of your own, so it might be worth reading through.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9330143540669856\n",
      "Test accuracy: 0.8555555555555555\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(train_cat.drop(target, axis=1), train_cat[target])\n",
    "\n",
    "y_pred = clf.predict(train_cat.drop(target, axis=1))\n",
    "print(f'Train accuracy: {metrics.accuracy_score(y_pred, train_cat[target])}')\n",
    "\n",
    "y_pred = clf.predict(test_cat.drop(target, axis=1))\n",
    "print(f'Test accuracy: {metrics.accuracy_score(y_pred, test_cat[target])}')\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(train.drop(target, axis=1), train[target])\n",
    "\n",
    "y_pred = clf.predict(train.drop(target, axis=1))\n",
    "print(f'Train accuracy: {metrics.accuracy_score(y_pred, train[target])}')\n",
    "\n",
    "y_pred = clf.predict(test.drop(target, axis=1))\n",
    "print(f'Test accuracy: {metrics.accuracy_score(y_pred, test[target])}')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis questions [4 pts]\n",
    "\n",
    "Answer these questions about the results. Chapter 3.5 from Mitchell's 'Machine Learning' might contain some hints. Write your answers below each question in this cell.\n",
    "\n",
    "**Why does the training accuracy for numerical splits become 1.0 (i.e. a perfect fit), but remains lower for categorical splits?**\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "**Explain how it is possible that the testing accuracy using only the categorical variables is higher than the testing accuracy using the categorical and numeric variables combined. What property of the Decision Tree causes this?**\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "**While using categorical data as if it is numerical is generally not recommended, in Decision Trees it doesn't seem as problematic. In fact, we could always reproduce a discrete split using a combination of numeric splits on that same variable. Explain how many splits we would need and what the structure might look like.**\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "**Aside from just being possible, here the results of using the Numerical Decision Tree of scikit-learn actually seem much better on catagorical data than those of an actual Catagorical Decision Tree. Give a hypothesis on what property of the data might cause this and motivate it with a logical argument.**\n",
    "\n",
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing overfitting [1 pt]\n",
    "\n",
    "As we now know, trees learn by beginning with **the full training set** and greedily adding conditions that maximize each child nodes' label purity. As the tree grows, each node adds another condition resulting in smaller subgroups. In fact, the number of leaf-nodes grows exponentially as we add more depth to the tree and in just a few splits a training set with thousands of examples can be reduced to a set of leaves with sizes around 1-20, which lack statistical validity. Since our implementation will continue separating nodes until they are either pure or no more conditions can be set to further improve purity, it is very probable that we force our tree to overfit.\n",
    "\n",
    "There are several techniques that deal with tree overfitting. It is possible to prune the tree, for example by removing leaves once the tree is complete, or by preventing it from growing as deep in the first place by setting a threshold on the minimum Information Gain. For more detail on this, read section **3.7.1** from Mitchell's 'Machine Learning'.\n",
    "\n",
    "Scikit-learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) has several argument options to help prevent overfitting. Probably the easiest to get started with is `min_samples_split`, which sets what the minimum number of samples in a node should be before it can be split. If a node has less samples than this number, it will not be split further and the predicted label just becomes the most common label at that node.\n",
    "\n",
    "Try several values for `min_samples_split` and give a value that definitely still overfits, that definitely causes underfitting and the value you think gives the best fit on the data. Report the training and testing accuracy for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum sample split: 3\n",
      "Train accuracy: 0.9760765550239234\n",
      "Test accuracy: 0.7333333333333333\n",
      "\n",
      "Minimum sample split: 10\n",
      "Train accuracy: 0.9234449760765551\n",
      "Test accuracy: 0.7222222222222222\n",
      "\n",
      "Minimum sample split: 20\n",
      "Train accuracy: 0.8755980861244019\n",
      "Test accuracy: 0.7333333333333333\n",
      "\n",
      "Minimum sample split: 30\n",
      "Train accuracy: 0.8660287081339713\n",
      "Test accuracy: 0.7666666666666667\n",
      "\n",
      "Minimum sample split: 35\n",
      "Train accuracy: 0.861244019138756\n",
      "Test accuracy: 0.7777777777777778\n",
      "\n",
      "Minimum sample split: 40\n",
      "Train accuracy: 0.861244019138756\n",
      "Test accuracy: 0.7777777777777778\n",
      "\n",
      "Minimum sample split: 50\n",
      "Train accuracy: 0.8086124401913876\n",
      "Test accuracy: 0.6888888888888889\n",
      "\n",
      "Minimum sample split: 100\n",
      "Train accuracy: 0.7942583732057417\n",
      "Test accuracy: 0.7\n",
      "\n",
      "Minimum sample split: 300\n",
      "Train accuracy: 0.5358851674641149\n",
      "Test accuracy: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "for minimum_split in [3, 10, 20, 30, 35, 40, 50, 100, 300]:\n",
    "    d_tree = DecisionTreeClassifier(min_samples_split=minimum_split)\n",
    "    d_tree.fit(train.drop(target, axis=1), train[target])\n",
    "    \n",
    "    print('\\nMinimum sample split:', minimum_split)\n",
    "    y_pred = d_tree.predict(train.drop(target, axis=1))\n",
    "    print(f'Train accuracy: {metrics.accuracy_score(y_pred, train[target])}')\n",
    "    \n",
    "    y_pred = d_tree.predict(test.drop(target, axis=1))\n",
    "    print(f'Test accuracy: {metrics.accuracy_score(y_pred, test[target])}')\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which minimum split size gives the best fit on the data? Explain why you choose this value.**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Decision Tree [1 pt]\n",
    "\n",
    "Sklearn also comes with a tool that can actually plot the whole decision tree. However, plotting the complete tree would be a bit hard to read, as it wouldn't really fit in the figure. We can again use another `DecisionTreeClassifier` argument to limit the size of the tree. Here using `max_depth` makes the most sense, as we want uniformly cut the tree at certain depth for the plot.\n",
    "\n",
    "Fit a decision tree of a limited depth and call it `d_tree`. The code below will already plot it. Make sure it the tree small enough so you can read off the values in the plot. If not, reduce the `max_depth` further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "d_tree = DecisionTreeClassifier(max_depth=2)\n",
    "d_tree.fit(train.drop(target, axis=1), train[target])\n",
    "### END SOLUTION\n",
    "\n",
    "figure(figsize=(15, 10))\n",
    "tree.plot_tree(d_tree, feature_names=df.columns.values,\n",
    "               class_names=['Healthy', 'Heart Disease'], impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to read out the boundaries set in the decision tree. Refer back to the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) if you don't know what a variable means. What parameters seem to be the best indicators for heart disease?**\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "**Explain why considering a very reduced version of the tree should still give some insight in the important factors of heart disease.**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests [4 pts]\n",
    "\n",
    "A method for reducing overfitting that is not discussed in Mitchell's book is the ensemble approach. In an ensemble approach multiple models are trained on (variations of) the original dataset, and then used to give a combined prediction. A commonly used ensemble method for decision trees is Random Forest. The essence of Random Forest classification is to have many decision trees that have all been trained on a random subset of features with randomly drawn sample rows. These decision trees then all predict on unseen samples and a majority vote is used as the final prediciton. This procedure usually leads to better model performance; while the predictions of a single tree are highly sensitive to noise in the training data; the average of many trees trained on very similar data is not.\n",
    "\n",
    "We will be implementing a simple but effective variant Random Forest that creates a small sample of all training data and trains several of scikit's `DecisionTreeClassifier`s on a random subset of features. First, we will need to create the forest. Implement `create_forest` that should accept `n_trees` the number of trees that should be in the forest, and returns a list containing all of the `DecisionTreeClassifier` objects. Check the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) for a way to change the maximum amount of features used in the tree to the square root of the number of features and use it when creating the classifiers. *This is an easy way to limit the number of features used in the classifier and will help to differentiate all the models learned by each of the trees.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forest(n_trees):\n",
    "    ### BEGIN SOLUTION\n",
    "    return [DecisionTreeClassifier(max_features='auto') for _ in range(n_trees)]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the function `train_forest` that accepts a `forest` (a list of `DecisionTreeClassifier` objects) and trains each of the trees with `ratio` samples from `data`. *Training each of the trees on different random subsets of the data will also help to differentiate all the models learned by each of the trees.*\n",
    "\n",
    "Hint: use the pandas `sample` method to create your subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forest(forest, data, target, ratio=0.5):\n",
    "    ### BEGIN SOLUTION\n",
    "    for tree in forest:\n",
    "        temp_data = data.sample(int(len(data) * ratio))\n",
    "        tree.fit(temp_data.drop(target, axis=1), temp_data[target])\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `predict_forest` that accepts a `forest` and predicts the target label for the passed `data`.\n",
    "\n",
    "Hint: since we only have two possible predictions, `True` and `False`, which are equal to 1 and 0 respectively, we can just add all predictions of all trees together, adding 1 or 0 for each. If we then divide the total prediction by the number of trees, we end up with an 'average vote' that can be rounded to be either 1 or 0; `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forest(forest, data):\n",
    "    ### BEGIN SOLUTION\n",
    "    pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    for tree in forest:\n",
    "        pred += tree.predict(data)\n",
    "        \n",
    "    return (pred / len(forest)) >= 0.5\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a random forest with 1000 trees, each using only 3 or 4 random features ($\\approx \\sqrt{13}$) and train each forest with a 50% random sample of the complete training data. Print out the train and testing accuracy. Does this result improve over using just 1 tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "n_trees = 1000\n",
    "\n",
    "forest = create_forest(n_trees)\n",
    "\n",
    "train_forest(forest, train, target, 0.5)\n",
    "\n",
    "train_pred = predict_forest(forest, train.drop(target, axis=1))\n",
    "test_pred = predict_forest(forest, test.drop(target, axis=1))\n",
    "\n",
    "print(f'Train accuracy: {metrics.accuracy_score(train_pred, train[target])}')\n",
    "print(f'Test accuracy: {metrics.accuracy_score(test_pred, test[target])}')\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
